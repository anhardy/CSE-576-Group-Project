{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code was based on Homework 1. I'm still in the process of fine-tuning it but I wanted a basic piece of code that could be integrated with the placeholder approach that we were working on.\n",
        "\n"
      ],
      "metadata": {
        "id": "M_npVihHGc0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbKTQnijJSHl",
        "outputId": "aa3359d5-984f-46d1-9397-c355b959461d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55i7IGD8FALv",
        "outputId": "b690a586-e91b-4ab9-ec8c-6096d200ff67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "j1TJBj1cFESu",
        "outputId": "0fca3c26-28b0-4d1e-c6d1-fcb510ca1e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 47,499\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       label                                             review\n",
              "13061     20  Well, I do wish I had had gloves such as these...\n",
              "23148     32  The plot of THE PERFECT SCORE is easy enough: ...\n",
              "43162     15  As with most AKB48 end of the year singles, th...\n",
              "4464      71  We all know the story of Alice in Wonderland. ...\n",
              "32977     77  This book is about the Guthrie Theater in Minn...\n",
              "29013     60  I didn't buy this for my granddaughter though ...\n",
              "41936     72  This viewer rented the DVD form of AMERICAN GA...\n",
              "30672     31  St. Martin's Press and NetGalley provided me w...\n",
              "17452     25  Only found in the northern hemisphere, there a...\n",
              "19955      5  Undine Spragg is considered to be one of liter..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c52afb44-a305-4879-aef8-cc8aefd3e36e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13061</th>\n",
              "      <td>20</td>\n",
              "      <td>Well, I do wish I had had gloves such as these...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23148</th>\n",
              "      <td>32</td>\n",
              "      <td>The plot of THE PERFECT SCORE is easy enough: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43162</th>\n",
              "      <td>15</td>\n",
              "      <td>As with most AKB48 end of the year singles, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4464</th>\n",
              "      <td>71</td>\n",
              "      <td>We all know the story of Alice in Wonderland. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32977</th>\n",
              "      <td>77</td>\n",
              "      <td>This book is about the Guthrie Theater in Minn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29013</th>\n",
              "      <td>60</td>\n",
              "      <td>I didn't buy this for my granddaughter though ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41936</th>\n",
              "      <td>72</td>\n",
              "      <td>This viewer rented the DVD form of AMERICAN GA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30672</th>\n",
              "      <td>31</td>\n",
              "      <td>St. Martin's Press and NetGalley provided me w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17452</th>\n",
              "      <td>25</td>\n",
              "      <td>Only found in the northern hemisphere, there a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19955</th>\n",
              "      <td>5</td>\n",
              "      <td>Undine Spragg is considered to be one of liter...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c52afb44-a305-4879-aef8-cc8aefd3e36e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c52afb44-a305-4879-aef8-cc8aefd3e36e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c52afb44-a305-4879-aef8-cc8aefd3e36e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(r'/content/drive/MyDrive/NLP_Fall2022/Group Project/DatasetSplit/train_split.xlsx', usecols=['label','review'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        " \n",
        " \n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "4969aebe44e847919be543832d4ad315",
            "096919bad2124706945f1eb3a23a8b59",
            "d02f5380531842eeb32aed7d19619191",
            "fb8a825f08274622b27630b692f3e391",
            "8516e09454ef469f936a3de29852313e",
            "3d39b5f4647a4f8997c18ce131f7b19d",
            "394fb61cf02245d0a8898373130d5575",
            "887702e5c2524db6959786970b4db6ea",
            "e1d1e23e16c34f6eb9f11e6e43e451ac",
            "1a71e293a98041dbbe48cc9211c384e6",
            "939f42a01d87450b8a6606b6bd6d0a29",
            "b269a0ac4f104b6bbb13edd4d2242a9f",
            "3759e05c95404a369b48d21d5a5dfa4d",
            "ee20ccb6f89b4092bc876e15f57f0134",
            "2f50cbc5bdce46768e769fc567a7adfb",
            "9350128afb80485f89c45251b62978eb",
            "3b578ce557b24078a8a58c2d0ddaa43c",
            "95667facad5d4d578966b09089bec9f6",
            "7566f8c846f541aeb90d8a197a99fcd6",
            "548392dcb5de4348bf5fcd889734480d",
            "ee8cc6a0eb5a4a20a92e85b60ebf8940",
            "431b6aa8f4934bbaa23022861738e3cb",
            "42060f409d164d0895154ef2b36f53bd",
            "58d7d23aa7d448439fb7987e6fa610f5",
            "1f9df558724d40ecadb90492ea1a09ab",
            "d290ec0df19742a7b1511e1012db246c",
            "ea274e5a6f514a7ea46a71717f7dcfd7",
            "732877d12ef04c07b4a5669cf5b0ed31",
            "25d9455f2b94428096903b1da3705b6c",
            "085372f681694020b12c55ea1017525f",
            "6bc5702cf040423594d2448de52c1793",
            "2eca7c4c21954e78a031c8d884146b30",
            "6fdf6475ef484b1189635dd9aedb4a54"
          ]
        },
        "id": "yHVIvs76GA5E",
        "outputId": "48603947-1915-4bcc-9be9-cfad2e3b707b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RobertaTokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4969aebe44e847919be543832d4ad315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b269a0ac4f104b6bbb13edd4d2242a9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42060f409d164d0895154ef2b36f53bd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading RobertaTokenizer...')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrVIj0z0RX4H",
        "outputId": "dd4a2a4c-634b-4868-8d10-474142a66f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,   713,    16,    10,  1531,   177,     4, 18609,     7,   278,\n",
            "            62,    24,    16, 15295,     4,    20, 11255,  4745,    16,  5342,\n",
            "          4375,     8,     5, 34634,  2045, 15481,   156,     4,   370,   109,\n",
            "            33,  1735,    15,   141,     7,   310,    61,    16,   205,   187,\n",
            "            24,    64,    28, 45212,   858,  3923,     7,   120,     5,  9553,\n",
            "            88,     5, 11255,     4,    85,    16,    98,  2007,     7,  6356,\n",
            "            62,    38,  2220,     2]])\n",
            "Token IDs: tensor([    0,   713,    16,    10,  1531,   177,     4, 18609,     7,   278,\n",
            "           62,    24,    16, 15295,     4,    20, 11255,  4745,    16,  5342,\n",
            "         4375,     8,     5, 34634,  2045, 15481,   156,     4,   370,   109,\n",
            "           33,  1735,    15,   141,     7,   310,    61,    16,   205,   187,\n",
            "           24,    64,    28, 45212,   858,  3923,     7,   120,     5,  9553,\n",
            "           88,     5, 11255,     4,    85,    16,    98,  2007,     7,  6356,\n",
            "           62,    38,  2220,     2])\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(df['review'][val], add_special_tokens=True, truncation=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        df['review'][val],                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "labelToList = list(df['label'])\n",
        "\n",
        "labels=[]\n",
        "\n",
        "for labelVal in labelToList:\n",
        "  labels.append(labelVal)\n",
        "print(input_ids[0])\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B78YZfmvhHsG",
        "outputId": "af8338cb-3bc9-40bb-da1b-fd4278a8169f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([47499, 64])\n",
            "torch.Size([47499, 64])\n",
            "tensor([64, 86, 59,  ..., 96, 60,  5])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids.size())\n",
        "print(attention_masks.size())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf5n2uZRMTfv",
        "outputId": "72ac906f-7cb4-4b2f-c4f9-79684c7b65b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47,499 training samples\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "# train_size = int(0.9 * len(dataset))\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(len(train_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VkNCdrxQ_EY",
        "outputId": "3123b4cd-c7db-443c-cde4-d19dfd65cf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Number of training sentences: 2,500\n",
            "\n",
            "Max sentence length:  512\n",
            "tensor([[    0,   713,    16,    10, 10262,   205,    78,  5808,     4,  1437,\n",
            "           616,     5,  2236,  4428,   101,    14,     9,    10,   239,   334,\n",
            "          1816,     6,     5, 24587,     6,  9817, 21930,     6,    16,    10,\n",
            "           706,    12,   180,    12,   279,   784, 32916,    54,    21,   553,\n",
            "             7,  3116,     5,  6168,   405, 16705,     9,    41,   793,  1692,\n",
            "           334,  1441,    54,  2021,  4260,     6,  2875,   957,     4,  1437,\n",
            "           152,  3685,  5699,     2]])\n",
            "2500\n",
            "2500\n",
            "2500\n",
            "2,500 validation samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(r'/content/drive/MyDrive/NLP_Fall2022/Group Project/DatasetSplit/test_split.xlsx', usecols=['label','review'])\n",
        "\n",
        " \n",
        " \n",
        "# for index, row in df.iterrows():\n",
        "#     # print(row['label'])\n",
        "#     if row['label'] not in [\"contradiction\", \"entailment\", \"neutral\"]:\n",
        "#         df.drop(index, inplace=True)\n",
        "\n",
        "df = df.reset_index()\n",
        "print(\" \")\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "#  +\" \"+df['sentence2']\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(df['review'][val], add_special_tokens=True, truncation=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        df['review'][val],                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "labelToList = list(df['label'])\n",
        "\n",
        "labels=[]\n",
        "\n",
        "for labelVal in labelToList:\n",
        "    labels.append(labelVal)\n",
        "print(input_ids[0])\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', sentences[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "\n",
        "print(len(input_ids))\n",
        "print(len(attention_masks))\n",
        "print(len(labels))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "val_size = len(val_dataset)\n",
        "\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm3LdxVwO8sV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f35e67e07531499e91e85d539d486ce3",
            "2921150ad6e34551a4e5fa7f9064caeb",
            "9e76fea7a493463ab4abffbe3f0a25af",
            "684bbe50d5ce45699694360c0f77c162",
            "844591b0d8864f9081536ec03bd0e550",
            "53964fbcf0164b4e915265e644534461",
            "8fd81671d387412b99f3a0eac97b7384",
            "7507a99cc17746f2a2d332e67a9fa25d",
            "a2c77ff7c76f4b628a08d0e57913889d",
            "15daeab1079b425790b9d09d304c010c",
            "a67ca474f75c41d0a65f8fbf83591c20"
          ]
        },
        "id": "gFsCTp_mporB",
        "outputId": "b807b5d3-0741-45ef-b3b3-eba9d7d133f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35e67e07531499e91e85d539d486ce3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from transformers import RobertaForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 100, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PIiVlDYCtSq",
        "outputId": "f9b0d926-09ae-4bcb-a8d7-549beccd1f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                (100, 768)\n",
            "classifier.out_proj.bias                                      (100,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLs72DuMODJO",
        "outputId": "f907b103-4fea-4576-d33c-40377deb4be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M38YPeQkeh-P",
        "outputId": "60c996db-39d0-4b19-c4cc-d97b37de986e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([44, 44, 44,  ..., 46, 46, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "61a083b8-d84a-43e4-a2b5-3d445ba52da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,485.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  1,485.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  1,485.    Elapsed: 0:00:45.\n",
            "  Batch   160  of  1,485.    Elapsed: 0:00:59.\n",
            "  Batch   200  of  1,485.    Elapsed: 0:01:13.\n",
            "  Batch   240  of  1,485.    Elapsed: 0:01:28.\n",
            "  Batch   280  of  1,485.    Elapsed: 0:01:42.\n",
            "  Batch   320  of  1,485.    Elapsed: 0:01:57.\n",
            "  Batch   360  of  1,485.    Elapsed: 0:02:11.\n",
            "  Batch   400  of  1,485.    Elapsed: 0:02:26.\n",
            "  Batch   440  of  1,485.    Elapsed: 0:02:40.\n",
            "  Batch   480  of  1,485.    Elapsed: 0:02:55.\n",
            "  Batch   520  of  1,485.    Elapsed: 0:03:09.\n",
            "  Batch   560  of  1,485.    Elapsed: 0:03:24.\n",
            "  Batch   600  of  1,485.    Elapsed: 0:03:38.\n",
            "  Batch   640  of  1,485.    Elapsed: 0:03:53.\n",
            "  Batch   680  of  1,485.    Elapsed: 0:04:07.\n",
            "  Batch   720  of  1,485.    Elapsed: 0:04:22.\n",
            "  Batch   760  of  1,485.    Elapsed: 0:04:36.\n",
            "  Batch   800  of  1,485.    Elapsed: 0:04:51.\n",
            "  Batch   840  of  1,485.    Elapsed: 0:05:05.\n",
            "  Batch   880  of  1,485.    Elapsed: 0:05:19.\n",
            "  Batch   920  of  1,485.    Elapsed: 0:05:34.\n",
            "  Batch   960  of  1,485.    Elapsed: 0:05:48.\n",
            "  Batch 1,000  of  1,485.    Elapsed: 0:06:03.\n",
            "  Batch 1,040  of  1,485.    Elapsed: 0:06:17.\n",
            "  Batch 1,080  of  1,485.    Elapsed: 0:06:32.\n",
            "  Batch 1,120  of  1,485.    Elapsed: 0:06:46.\n",
            "  Batch 1,160  of  1,485.    Elapsed: 0:07:01.\n",
            "  Batch 1,200  of  1,485.    Elapsed: 0:07:15.\n",
            "  Batch 1,240  of  1,485.    Elapsed: 0:07:29.\n",
            "  Batch 1,280  of  1,485.    Elapsed: 0:07:44.\n",
            "  Batch 1,320  of  1,485.    Elapsed: 0:07:58.\n",
            "  Batch 1,360  of  1,485.    Elapsed: 0:08:13.\n",
            "  Batch 1,400  of  1,485.    Elapsed: 0:08:27.\n",
            "  Batch 1,440  of  1,485.    Elapsed: 0:08:42.\n",
            "  Batch 1,480  of  1,485.    Elapsed: 0:08:56.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epcoh took: 0:08:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 1.29\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,485.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,485.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,485.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,485.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,485.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,485.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,485.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,485.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,485.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,485.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,485.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,485.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,485.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,485.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,485.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,485.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,485.    Elapsed: 0:04:05.\n",
            "  Batch   720  of  1,485.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,485.    Elapsed: 0:04:34.\n",
            "  Batch   800  of  1,485.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,485.    Elapsed: 0:05:03.\n",
            "  Batch   880  of  1,485.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,485.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,485.    Elapsed: 0:05:46.\n",
            "  Batch 1,000  of  1,485.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,485.    Elapsed: 0:06:15.\n",
            "  Batch 1,080  of  1,485.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,485.    Elapsed: 0:06:44.\n",
            "  Batch 1,160  of  1,485.    Elapsed: 0:06:59.\n",
            "  Batch 1,200  of  1,485.    Elapsed: 0:07:13.\n",
            "  Batch 1,240  of  1,485.    Elapsed: 0:07:27.\n",
            "  Batch 1,280  of  1,485.    Elapsed: 0:07:42.\n",
            "  Batch 1,320  of  1,485.    Elapsed: 0:07:56.\n",
            "  Batch 1,360  of  1,485.    Elapsed: 0:08:11.\n",
            "  Batch 1,400  of  1,485.    Elapsed: 0:08:25.\n",
            "  Batch 1,440  of  1,485.    Elapsed: 0:08:40.\n",
            "  Batch 1,480  of  1,485.    Elapsed: 0:08:54.\n",
            "\n",
            "  Average training loss: 0.75\n",
            "  Training epcoh took: 0:08:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,485.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,485.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,485.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,485.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,485.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,485.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,485.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,485.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,485.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,485.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,485.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,485.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,485.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,485.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,485.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,485.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,485.    Elapsed: 0:04:05.\n",
            "  Batch   720  of  1,485.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,485.    Elapsed: 0:04:34.\n",
            "  Batch   800  of  1,485.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,485.    Elapsed: 0:05:03.\n",
            "  Batch   880  of  1,485.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,485.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,485.    Elapsed: 0:05:46.\n",
            "  Batch 1,000  of  1,485.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,485.    Elapsed: 0:06:15.\n",
            "  Batch 1,080  of  1,485.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,485.    Elapsed: 0:06:44.\n",
            "  Batch 1,160  of  1,485.    Elapsed: 0:06:59.\n",
            "  Batch 1,200  of  1,485.    Elapsed: 0:07:13.\n",
            "  Batch 1,240  of  1,485.    Elapsed: 0:07:28.\n",
            "  Batch 1,280  of  1,485.    Elapsed: 0:07:42.\n",
            "  Batch 1,320  of  1,485.    Elapsed: 0:07:57.\n",
            "  Batch 1,360  of  1,485.    Elapsed: 0:08:11.\n",
            "  Batch 1,400  of  1,485.    Elapsed: 0:08:25.\n",
            "  Batch 1,440  of  1,485.    Elapsed: 0:08:40.\n",
            "  Batch 1,480  of  1,485.    Elapsed: 0:08:54.\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epcoh took: 0:08:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,485.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,485.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,485.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,485.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,485.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,485.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,485.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,485.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,485.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,485.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,485.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,485.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,485.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,485.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,485.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,485.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,485.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,485.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,485.    Elapsed: 0:04:34.\n",
            "  Batch   800  of  1,485.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,485.    Elapsed: 0:05:03.\n",
            "  Batch   880  of  1,485.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,485.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,485.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,485.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,485.    Elapsed: 0:06:15.\n",
            "  Batch 1,080  of  1,485.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,485.    Elapsed: 0:06:44.\n",
            "  Batch 1,160  of  1,485.    Elapsed: 0:06:59.\n",
            "  Batch 1,200  of  1,485.    Elapsed: 0:07:13.\n",
            "  Batch 1,240  of  1,485.    Elapsed: 0:07:28.\n",
            "  Batch 1,280  of  1,485.    Elapsed: 0:07:42.\n",
            "  Batch 1,320  of  1,485.    Elapsed: 0:07:57.\n",
            "  Batch 1,360  of  1,485.    Elapsed: 0:08:11.\n",
            "  Batch 1,400  of  1,485.    Elapsed: 0:08:25.\n",
            "  Batch 1,440  of  1,485.    Elapsed: 0:08:40.\n",
            "  Batch 1,480  of  1,485.    Elapsed: 0:08:54.\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epcoh took: 0:08:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:36:21 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # print(\"batch\")\n",
        "        # print(b_input_ids)\n",
        "        # print(\"mask\")\n",
        "        # print(b_input_mask)\n",
        "        # print(\"labels\")\n",
        "        # print(b_labels)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        # print(b_input_ids)\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6O_NbXFGMukX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fb09a2d2-480a-4fec-e6f1-d65965e4181d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               1.08         1.29           0.68       0:08:58         0:00:09\n",
              "2               0.75         1.22           0.70       0:08:56         0:00:09\n",
              "3               0.82         1.22           0.70       0:08:56         0:00:09\n",
              "4               0.82         1.22           0.70       0:08:56         0:00:09"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bafbc02-69da-4074-af90-d6e23d6879b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.08</td>\n",
              "      <td>1.29</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0:08:58</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0:08:56</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.82</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0:08:56</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.82</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0:08:56</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bafbc02-69da-4074-af90-d6e23d6879b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4bafbc02-69da-4074-af90-d6e23d6879b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4bafbc02-69da-4074-af90-d6e23d6879b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "d8f9e422-e490-424d-8a93-7df5ad1721e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVxVdf7H8fe9lwvIJougyKZpoCHgbpqTaaWmlqWmTi5j2ZSVU+NvKjVr5ldNNZVOu/nLmsolzdTUTM1cy0wd0TQVrXABXBFkVdnu/f2B3iKwLgqcC7yej0ePR/d77jnnc458683X7/kek91utwsAAACAYcxGFwAAAADUd4RyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAdVZaWppiYmL0xhtvXPYxJk2apJiYmCqsqu661P2OiYnRpEmTnDrGG2+8oZiYGKWlpVV5fYsXL1ZMTIy2bt1a5ccGgCvlZnQBAOqPyoTbtWvXKjw8vBqrqX3Onj2rGTNmaMWKFTp16pQCAwPVoUMHPfjgg2rRooVTx3j44Yf1xRdfaMmSJWrdunWF37Hb7brxxhuVk5OjTZs2ydPTsyovo1pt3bpV27Zt05/+9Cf5+fkZXU45aWlpuvHGGzVixAj9/e9/N7ocAC6EUA6gxrz00ktlPicmJurjjz/WsGHD1KFDhzLbAgMDr/h8YWFh2r17tywWy2Uf49lnn9XTTz99xbVUhSeffFKff/65BgwYoM6dOys9PV3r1q3Trl27nA7lQ4YM0RdffKFFixbpySefrPA7W7Zs0dGjRzVs2LAqCeS7d++W2VwzfzG7bds2vfnmm7rjjjvKhfKBAweqf//+slqtNVILAFQGoRxAjRk4cGCZzyUlJfr444/Vtm3bctt+LS8vTz4+PpU6n8lkkoeHR6Xr/CVXCXDnzp3TqlWr1L17d02bNs3RPn78eBUWFjp9nO7duys0NFSfffaZHn/8cbm7u5f7zuLFiyWVBviqcKV/BlXFYrFc0S9oAFCdmFMOwOX06tVLo0aN0r59+zR27Fh16NBBt912m6TScP7KK6/ozjvvVJcuXdSmTRvdfPPNmjp1qs6dO1fmOBXNcf5l2/r16zV48GDFxcWpe/fuevHFF1VcXFzmGBXNKb/Ylpubq3/84x/q2rWr4uLiNHz4cO3atavc9Zw5c0aTJ09Wly5d1K5dO40ePVr79u3TqFGj1KtXL6fuiclkkslkqvCXhIqC9aWYzWbdcccdysrK0rp168ptz8vL0+rVqxUdHa34+PhK3e9LqWhOuc1m0//93/+pV69eiouL04ABA7Rs2bIK909OTtb//u//qn///mrXrp0SEhI0aNAgffLJJ2W+N2nSJL355puSpBtvvFExMTFl/vwvNac8MzNTTz/9tHr06KE2bdqoR48eevrpp3XmzJky37u4/7fffqv33ntPN910k9q0aaM+ffro008/depeVMb+/fv10EMPqUuXLoqLi1O/fv00c+ZMlZSUlPne8ePHNXnyZPXs2VNt2rRR165dNXz48DI12Ww2ffDBB7r11lvVrl07tW/fXn369NETTzyhoqKiKq8dQOUxUg7AJR07dkx/+tOf1LdvX/Xu3Vtnz56VJJ08eVILFy5U7969NWDAALm5uWnbtm169913lZSUpPfee8+p42/cuFEfffSRhg8frsGDB2vt2rX6z3/+o4YNG2rcuHFOHWPs2LEKDAzUQw89pKysLL3//vu67777tHbtWseofmFhoe6++24lJSVp0KBBiouL04EDB3T33XerYcOGTt8PT09P3X777Vq0aJGWL1+uAQMGOL3vrw0aNEhvv/22Fi9erL59+5bZ9vnnn+v8+fMaPHiwpKq737/2wgsvaNasWerUqZPGjBmjjIwMPfPMM4qIiCj33W3btmn79u264YYbFB4e7vhbgyeffFKZmZm6//77JUnDhg1TXl6evvzyS02ePFkBAQGSfvtZhtzcXP3xj3/UkSNHNHjwYF1zzTVKSkrSvHnztGXLFn3yySfl/obmlVde0fnz5zVs2DC5u7tr3rx5mjRpkiIjI8tNw7pc33//vUaNGiU3NzeNGDFCjRo10vr16zV16lTt37/f8bclxcXFuvvuu3Xy5EndddddatasmfLy8nTgwAFt375dd9xxhyTp7bff1uuvv66ePXtq+PDhslgsSktL07p161RYWOgyfyME1Gt2ADDIokWL7NHR0fZFixaVae/Zs6c9OjravmDBgnL7FBQU2AsLC8u1v/LKK/bo6Gj7rl27HG2pqan26Oho++uvv16uLSEhwZ6amupot9ls9v79+9uvu+66MsedOHGiPTo6usK2f/zjH2XaV6xYYY+OjrbPmzfP0TZnzhx7dHS0ffr06WW+e7G9Z8+e5a6lIrm5ufY///nP9jZt2tivueYa++eff+7UfpcyevRoe+vWre0nT54s0z506FB7bGysPSMjw263X/n9ttvt9ujoaPvEiRMdn5OTk+0xMTH20aNH24uLix3te/bsscfExNijo6PL/Nnk5+eXO39JSYl95MiR9vbt25ep7/XXXy+3/0UXf962bNniaPv3v/9tj46Ots+ZM6fMdy/++bzyyivl9h84cKC9oKDA0X7ixAl7bGysfcKECeXO+WsX79HTTz/9m98bNmyYvXXr1vakpCRHm81msz/88MP26Oho++bNm+12u92elJRkj46Otr/zzju/ebzbb7/dfsstt/xufQCMw/QVAC7J399fgwYNKtfu7u7uGNUrLi5Wdna2MjMz1a1bN0mqcPpIRW688cYyq7uYTCZ16dJF6enpys/Pd+oYY8aMKfP52muvlSQdOXLE0bZ+/XpZLBaNHj26zHfvvPNO+fr6OnUem82mRx55RPv379fKlSt1/fXX69FHH9Vnn31W5ntPPfWUYmNjnZpjPmTIEJWUlGjJkiWOtuTkZH333Xfq1auX40Hbqrrfv7R27VrZ7XbdfffdZeZ4x8bG6rrrriv3fS8vL8e/FxQU6MyZM8rKytJ1112nvLw8HTx4sNI1XPTll18qMDBQw4YNK9M+bNgwBQYGas2aNeX2ueuuu8pMGWrcuLGaN2+uw4cPX3Ydv5SRkaGdO3eqV69eatWqlaPdZDLpgQcecNQtyfEztHXrVmVkZFzymD4+Pjp58qS2b99eJTUCqHpMXwHgkiIiIi75UN7cuXM1f/58/fTTT7LZbGW2ZWdnO338X/P395ckZWVlydvbu9LHuDhdIisry9GWlpamkJCQcsdzd3dXeHi4cnJyfvc8a9eu1aZNm/Tyyy8rPDxcr732msaPH6/HH39cxcXFjikKBw4cUFxcnFNzzHv37i0/Pz8tXrxY9913nyRp0aJFkuSYunJRVdzvX0pNTZUkXXXVVeW2tWjRQps2bSrTlp+frzfffFMrV67U8ePHy+3jzD28lLS0NLVp00ZubmX/d+jm5qZmzZpp37595fa51M/O0aNHL7uOX9ckSS1btiy37aqrrpLZbHbcw7CwMI0bN07vvPOOunfvrtatW+vaa69V3759FR8f79jvf/7nf/TQQw9pxIgRCgkJUefOnXXDDTeoT58+lXomAUD1IZQDcEkNGjSosP3999/Xv/71L3Xv3l2jR49WSEiIrFarTp48qUmTJslutzt1/N9aheNKj+Hs/s66+GBip06dJJUG+jfffFMPPPCAJk+erOLiYrVq1Uq7du3Sc88959QxPTw8NGDAAH300UfasWOHEhIStGzZMjVp0kR/+MMfHN+rqvt9Jf72t79pw4YNGjp0qDp16iR/f39ZLBZt3LhRH3zwQblfFKpbTS3v6KwJEyZoyJAh2rBhg7Zv366FCxfqvffe07333qvHHntMktSuXTt9+eWX2rRpk7Zu3aqtW7dq+fLlevvtt/XRRx85fiEFYBxCOYBaZenSpQoLC9PMmTPLhKOvvvrKwKouLSwsTN9++63y8/PLjJYXFRUpLS3NqRfcXLzOo0ePKjQ0VFJpMJ8+fbrGjRunp556SmFhYYqOjtbtt9/udG1DhgzRRx99pMWLFys7O1vp6ekaN25cmftaHff74kjzwYMHFRkZWWZbcnJymc85OTnasGGDBg4cqGeeeabMts2bN5c7tslkqnQthw4dUnFxcZnR8uLiYh0+fLjCUfHqdnFa1U8//VRu28GDB2Wz2crVFRERoVGjRmnUqFEqKCjQ2LFj9e677+qee+5RUFCQJMnb21t9+vRRnz59JJX+DcgzzzyjhQsX6t57763mqwLwe1zr130A+B1ms1kmk6nMCG1xcbFmzpxpYFWX1qtXL5WUlGjWrFll2hcsWKDc3FynjtGjRw9Jpat+/HK+uIeHh/7973/Lz89PaWlp6tOnT7lpGL8lNjZWrVu31ooVKzR37lyZTKZya5NXx/3u1auXTCaT3n///TLL++3du7dc0L74i8CvR+RPnTpVbklE6ef5585Oq7npppuUmZlZ7lgLFixQZmambrrpJqeOU5WCgoLUrl07rV+/Xj/88IOj3W6365133pEk3XzzzZJKV4/59ZKGHh4ejqlBF+9DZmZmufPExsaW+Q4AYzFSDqBW6du3r6ZNm6Y///nPuvnmm5WXl6fly5dXKozWpDvvvFPz58/Xq6++qpSUFMeSiKtWrVJUVFS5ddErct1112nIkCFauHCh+vfvr4EDB6pJkyZKTU3V0qVLJZUGrLfeekstWrTQLbfc4nR9Q4YM0bPPPquvv/5anTt3LjcCWx33u0WLFhoxYoTmzJmjP/3pT+rdu7cyMjI0d+5ctWrVqsw8bh8fH1133XVatmyZPD09FRcXp6NHj+rjjz9WeHh4mfn7kpSQkCBJmjp1qm699VZ5eHjo6quvVnR0dIW13HvvvVq1apWeeeYZ7du3T61bt1ZSUpIWLlyo5s2bV9sI8p49ezR9+vRy7W5ubrrvvvs0ZcoUjRo1SiNGjNBdd92l4OBgrV+/Xps2bdKAAQPUtWtXSaVTm5566in17t1bzZs3l7e3t/bs2aOFCxcqISHBEc779euntm3bKj4+XiEhIUpPT9eCBQtktVrVv3//arlGAJXjmv8XA4BLGDt2rOx2uxYuXKjnnntOwcHBuuWWWzR48GD169fP6PLKcXd314cffqiXXnpJa9eu1cqVKxUfH68PPvhAU6ZM0fnz5506znPPPafOnTtr/vz5eu+991RUVKSwsDD17dtX99xzj9zd3TVs2DA99thj8vX1Vffu3Z067q233qqXXnpJBQUF5R7wlKrvfk+ZMkWNGjXSggUL9NJLL6lZs2b6+9//riNHjpR7uPLll1/WtGnTtG7dOn366adq1qyZJkyYIDc3N02ePLnMdzt06KBHH31U8+fP11NPPaXi4mKNHz/+kqHc19dX8+bN0+uvv65169Zp8eLFCgoK0vDhw/WXv/yl0m+RddauXbsqXLnG3d1d9913n+Li4jR//ny9/vrrmjdvns6ePauIiAg9+uijuueeexzfj4mJ0c0336xt27bps88+k81mU2hoqO6///4y37vnnnu0ceNGzZ49W7m5uQoKClJCQoLuv//+Miu8ADCOyV4TT+kAAMooKSnRtddeq/j4+Mt+AQ8AoO5gTjkAVLOKRsPnz5+vnJycCtflBgDUP0xfAYBq9uSTT6qwsFDt2rWTu7u7du7cqeXLlysqKkpDhw41ujwAgAtg+goAVLMlS5Zo7ty5Onz4sM6ePaugoCD16NFDjzzyiBo1amR0eQAAF0AoBwAAAAzGnHIAAADAYIRyAAAAwGA86HnBmTP5stlqdiZPUJCPMjLyavScQG1EXwGcQ18BnGNUXzGbTQoI8K5wG6H8ApvNXuOh/OJ5Afw++grgHPoK4BxX6ytMXwEAAAAMRigHAAAADEYoBwAAAAxm6JzyU6dOadasWdq1a5f27Nmjs2fPatasWerSpcvv7vvhhx9q5cqVOnz4sPLz8xUaGqoePXrogQceUGBgYA1UDwAAAFQNQ0P5oUOHNHPmTEVFRSkmJkY7d+50et99+/bp6quvVt++feXt7a1Dhw5pwYIF+vrrr7VkyRJ5enpWY+UAAABA1TE0lMfGxmrLli0KCAjQmjVr9NBDDzm974svvliurW3btvrLX/6iDRs2qG/fvlVZKgAAAFBtDA3lPj4+VXq8pk2bSpJyc3Or9LgAAABAdar165RnZmaqpKRER44c0dSpU+Xm5qZOnToZXRYAAADgtFodyvPz89W1a1fH5yZNmmjatGlq1qyZcUUBAAAAlVSrQ7mnp6fef/99FRQUaP/+/Vq9erXy8i7vlalBQVU7lea3fH1km+btXqqMs5kK8grUH+MH6g9RnWvs/EBtFBzsa3QJQK1AXwGc42p9pVaHcovFom7dukmSevbsqW7dumno0KEKCgpSz549K3WsjIy8Gnnd6rYTO/TR/kUqshVJkk6fzdSMbXOUk3NOnZu0r/bzA7VRcLCv0tN5VgT4PfQVwDlG9RWz2XTJgeA69fKghIQEhYaG6rPPPjO6lEtalrzKEcgvKrIVaclPK2Sz2wyqCgAAAEaq1SPlFSkoKHDp1VfOFGRV2J5dmKPHvvpfRfqGKcovQpF+4YryjVCgp79MJlMNVwkAAICaVCtCeUpKiiQpMjJSUmnwLioqKrek4po1a5SZmanY2Ngar9FZAR7+FQZzbzcvtW+coCM5qVqX+rVK7CWSJF+rj6L8whXpF6Eo33BF+UXI173m5r8DAACg+hkeyqdPny5JSk5OliQtXbpUiYmJ8vPz08iRIyVJY8aMkSStW7dOkpSenq477rhDt9xyi1q0aCE3Nzft3btXy5YtU1hYmEaPHl3zF+Kk21r0LTOnXJKsZquGRN/mmFNeZCvWsbzjOpKTqsM5qTqSm6a9GQdkV+mc90DPAEdAj/KLUKRvmDzdeIMpAABAbWWy2+3V/3Tjb4iJiamwPSwszBHCe/XqJennUJ6Xl6d///vf2rp1q44dO6aioiKFhoaqR48eevDBBxUYGFjpOmrqQU+p9GHPZcmrlFWQJX8Pf93Wou/vPuR5vvi8UnOP6khumo7kpOpITpoyzmdKkkwyqbFXsGPaSzO/CIX5NJXVbPjvXECV4OE1wDn0FcA5rvigp+Gh3FXUZCi/6Ep/IHIL85Tyi5B+JDdVuYWlS0JaTBaF+TS5MO0lQlF+4Qr1biyzqU4924t6gqABOIe+AjjHFUM5Q6m1mK+7j2KDWik2qJUkyW63K6sgu3TKy4VpL9tPfKdNR7dIktzNVkX4hivK78LUF98INWoQyIOkAAAABiOU1yEmk0kBnv4K8PRXu5A4SZLNblP62dO/mPaSqq+Ofqvi1K8llT5gWrrSy89z1Bt6+Bl5GQAAAPUOobyOM5vMauwdosbeIY556yW2Eh3LP1Fm2svqlA2OddL9PRoqyvfCii8XAruX1cvIywAAAKjTCOX1kMVsUYRvmCJ8w9Q9rLStsKRQqbnHdCS3dDQ9JSdNu07vdewT0qBR6Yj6hWkvEb5N5W5xN+gKAAAA6hZCOSRJ7hZ3tfBvphb+zRxtZ4vOKiX3qA7npColJ1U/ZR3S9pPfSSodgQ/1bux4iDTKL0JNvZvIYrYYdAUAAAC1F6Ecl+Rl9VKrwKvVKvBqR1t2QY7jIdIjOan6Lv17bT6+TZJkNbsp3KepIv0i1OzCy46CvRqx4gsAAMDvIJSjUhp6+Ck+OFbxwaVvTbXb7Tp9LtMx7eVITpq+PbZNG9O+kSR5Wjx/9SBpuAI8/FnxBQAA4BcI5bgiJpNJwV5BCvYKUsfGbSWVPkh64uwpx0OkKTmpWpf6tUrsJZJKl3L85bSXKN8I+bh7G3kZAAAAhiKUo8pZzBaF+YQqzCdU3dRJklRUUqSj+cdLg/qFpRn3ZuyXXaUvbAryDCgz7SXCN0yebp5GXgYAAECNIZSjRlgtVjXzi1Qzv0hH27ni80rNPVpmjvrOU7slSSaZ1Ng7xDHtpZlfhJr6hMpq5kcWAADUPSQcGKaBm6eiA1ooOqCFoy23MM8R0lNyUrUv44C2nkiUJFlMpSPwURdG06P8ItTEO4QHSQEAQK1HKIdL8XX3UZtGrdWmUWtJpQ+SninIurAsY+lo+n9P7NDXR7+VVLqUY6RvWJk56kGegTxICgAAahVCOVyayWRSoGeAAj0D1D4kXpJks9t06uzpCyPqpSu+bDy6WcWpxZIkb6uXIn3DS+en+0Uo0jdCDT18jbwMAACA30QoR61jNpnVxDtETbxD1CW0gySp2FasY/kndCSndNrLkdw0rTq8zvEgqb9HwzLTXiJ9w+VlbWDkZQAAADgQylEnuJndFOkbrkjfcCnsWklSQUmhUnOPOkL6kZxU7Urf49gnxKvRhWkvpVNfwn3C5G6xGnUJAACgHiOUo87ysLirpX9ztfRv7mg7W3TWEdCP5KTphzPJ+u/JnZJKR+BDvRtfWJYxQpF+EWrq3VgWs8WoSwAAAPUEoRz1ipfVS60Do9U6MNrRllWQ7Zj2cjgnVTtPfa9vjm2TJFnNbgr3Cfv5RUd+EQpuEMSKLwAAoEoRylHv+Xs0lH9wQyUEx0oqXfEl/VxGmWkvm49t04a0bySVLuUY6RteZo66v0dDVnwBAACXjVAO/IrJZFKIVyOFeDVSxybtJEklthKdOHvK8TbSI7lpWpOyUTa7TZLk5+5bOpp+YdpLlF+4fKzeRl4GAACoRQjlgBMs5tIXF4X5hKpb086SpKKSIqXlHS+zNOOe0/sdK74EeQaqmV+EIi+E9QjfMHm6eRh5GQAAwEURyoHLZLVY1bxhpJo3jHS0nSs+r9TcNB258KKjg9lHlHhqlyTJJJOaeIeUedFRmE+o3Mx0QwAA6jvSAFCFGrh5KjqgpaIDWjracgvzykx72ZORpC0ntkuS3EwWhfk0VZRfeOm0F99wNfEO4UFSAADqGUI5UM183X3UplFrtWnUWlLpg6SZ57MuTHkp/WfbiR366ui3kkqXcoz0DXdMe4nyi1CQZwAPkgIAUIcRyoEaZjKZFNQgQEENAtQ+JF6SZLPbdOpsuo7kpOnwhTnqG1O/UbG9RJLkY/X+ecWXC1Nf/Nx9jbwMAABQhQjlgAswm8xq4t1YTbwbq0toB0lSsa1Yx/JOOB4iPZKTqqTDPzgeJA3w8P/Fii/hivILVwO3BkZeBgAAuEyEcsBFuZndFOlXOo3lD2GlbQUlhUrNPVpmjvp36Xsc+zT2ClbkLx4kDfdpKneL1aArAAAAziKUA7WIh8VdLf2bq6V/c0dbXlG+UnIurPiSm6ofzvyo/57cIal0BD7Mu4lj7fQo3wiFejeWxWwx6hIAAEAFCOVALedj9dY1QTG6JijG0ZZVkH1hNL102suOU7v1zbGtkiSr2aoI36aOh0ij/MIV3KARD5ICAGAgQjlQB/l7NJR/cEMlBLeRVLriS/q5047R9CM5adp0bKvWp22SJDVwa6Coiyu++EWomV+E/D0aGnkJAADUK4RyoB4wmUwK8QpWiFewOjVpJ0kqsZXoeP5JR0hPyUnVmpSNstltkqSG7r4X1k7/eY66t9XLyMsAAKDOIpQD9ZTFbFG4b1OF+zbVdU27SJIKS4p0NO9Y6bKMOWlKyU3V96f3OfZp5Bl4YcpL6T8RvmHysLgbdQkAANQZhHIADu4Wq5o3jFLzhlGOtnPF55SSc9Qxon4w+4gST+2SJJlkUqh3Y8eLjpr5RaipTxO5mflPCwAAlcH/OQH8pgZuDRQT2FIxgS0dbTmFuT8/SJqbqj2nk7Tl+HZJkpvJojDHg6Sl014aewXLbDIbdQkAALg8QjmASvNz91Vco2sU1+gaSaUPkmacP3Nh7fRUpeSkaeuJ7frq6GZJkqfFQxG+YT9PffENV6BnACu+AABwgclut9uNLsIVZGTkyWar2VsRHOyr9PTcGj0nUFNsdptOnk3/+UVHOWk6mndMxfYSSaVLOUb6hauZ789z1H3dfcocY9uJHVqWvEpZBVny9/DXbS36qnOT9kZcDuDS6CuAc4zuK2azSUFBPhVuI5RfQCgHql+RrVjH8o47pr2k5KTpeP5J2VXa9wI8/B1rp58vPq91qZtUZCty7G81W3VXq8GEDeAXtp3YoY/2L6KvAL/DFfoKodwJhHLAGOeLC5Sae9QR0o/kpOr0+cxLft/dbHVMmwEgfX96nwp/ETIuoq8AZV2qrwR4+Ouf1z1RIzX8VihnTjkAQ3m6eejqgKt0dcBVjra8wnxN3PR0hd8vtBUpLe9YTZUHuLyKQsbFdvoK8LNL9ZUzBVk1XEnFCOUAXI6Pu7cCPPwr/A9lgIe//n7tYwZUBbimJ795nr4COOG3+oorYI0yAC7pthZ9ZTVby7RZzVbd1qKvQRUBrom+AjjH1fuKoSPlp06d0qxZs7Rr1y7t2bNHZ8+e1axZs9SlS5ff3M9ms+nTTz/Vl19+qaSkJGVnZys8PFwDBgzQPffcI3d33jAI1HYXH7phRQngt9FXAOe4el8x9EHPrVu3avTo0YqKilJgYKB27tzpVCjPz89X+/bt1bZtW91www0KCgrSzp07tWTJEnXp0kUffPBBpWvhQU/AddFXAOfQVwDnGNVXXPZBz9jYWG3ZskUBAQFas2aNHnroIaf2s1qtmjdvntq3//k3m6FDhyosLExvvPGGtm7d+rvBHgAAAHAVhs4p9/HxUUBAQKX3c3d3LxPIL7r55pslScnJyVdcGwAAAFBT6tSDnqdPn5akywr6AAAAgFHqVCh/99135evrq+7duxtdCgAAAOC0OrNO+YwZM7R582Y988wz8vX1rfT+l5p0X92CgytfK1Af0VcA59BXAOe4Wl+pE6F8xYoVevXVVzVs2DANGzbsso7B6iuA66KvAM6hrwDOccXVV2r99JVvvvlGjz/+uHr27Kl//OMfRpcDAAAAVFqtDuW7du3S+PHjFRcXp1deeUUWi8XokgAAAIBKqxWhPCUlRSkpKWXakpOTdd999yksLEwzZsyQp6enQdUBAAAAV8bwOeXTp0+X9PPa4kuXLlViYqL8/Pw0cuRISdKYMWMkSevWrZMk5eXlaezYscrJydHYsWO1YcOGMseMiQWkhgsAACAASURBVIlRq1atauYCAAAAgCtkeCh/7bXXynxetGiRJCksLMwRyn8tKytLx48flyRNmzat3Pbx48cTygEAAFBrGB7KDxw48LvfuThCflF4eLhT+wEAAAC1Qa2YUw4AAADUZYRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYIRyAAAAwGCEcgAAAMBghHIAAADAYG5GnvzUqVOaNWuWdu3apT179ujs2bOaNWuWunTp8rv7btq0SStWrND333+vn376SaGhoVq3bl0NVA0AAABULUNHyg8dOqSZM2fq5MmTiomJqdS+y5cv1/Lly+Xt7a3GjRtXU4UAAABA9TM0lMfGxmrLli1avXq17r333krtO2HCBCUmJmr+/Pm65pprqqlCAAAAoPoZOn3Fx8fnsvdldBwAAAB1BQ96AgAAAAYjlAMAAAAGM3T6iisJCrr8qTRXIjjY15DzArUNfQVwDn0FcI6r9RVC+QUZGXmy2ew1es7gYF+lp+fW6DmB2oi+AjiHvgI4x6i+YjabLjkQzPQVAAAAwGCEcgAAAMBgtSKUp6SkKCUlxegyAAAAgGph+Jzy6dOnS5KSk5MlSUuXLlViYqL8/Pw0cuRISdKYMWMkSevWrXPst3//fsfnw4cPKzc313GsTp06qVOnTjV1CQAAAMAVMTyUv/baa2U+L1q0SJIUFhbmCOUV2bdvX7l9L34eP348oRwAAAC1hslut9fskiMuitVXANdFXwGcQ18BnMPqKwAAAADKIZQDAAAABjN8Tnl99O3eE1q8MVmZOQUK9PPQoB4t1DW2idFlAQAAwCCE8hr27d4T+nDlfhUW2yRJGTkF+nDlfkkimAMAANRTTF+pYYs3JjsC+UWFxTYt3phsUEUAAAAwGqG8hmXkFFSqHQAAAHUfobyGBfl5VNju08Baw5UAAADAVRDKa9igHi3k7lb2tptMUt65In25PdWgqgAAAGAkHvSsYRcf5vzl6iu3dW+u7348rXlrflRG9nkN7dVSZpPJ4EoBAABQUwjlBuga20RdY5uUeZvUdW1CNW/Nj1r931Rl5hbozwNay+pmMbhSAAAA1ARCuYswm0266+arFdTQUwvW/6TsvAL9ZXA8c80BAADqAeaUuxCTyaS+XSI1bmCsDh3P0QtzEnU665zRZQEAAKCaEcpdUOfWjfW3YW2VnVeof85O1OETOUaXBAAAgGpEKHdRMZEBmjyqg6wWk16cu1O7k08bXRIAAACqCaHchYU18taU0R3VOLCBXl/4vTZ+d9TokgAAAFANCOUuzt/HQxPvaq9rmgfow1UHtPirg7Lb7UaXBQAAgCpEKK8FGni46eHB8fpDfKiWbz6s9z5PUnGJzeiyAAAAUEVYErGWcLOYNeaWVgry89SSTYeUlVegh+6IUwMP/ggBAABqO0bKaxGTyaTbujfXPf1a60BKll6Ys0NncguMLgsAAABXqEpCeXFxsb744gstWLBA6enpVXFI/Ibu8aF65M54nc4+p3/O2q609DyjSwIAAMAVqHQof+mllzR48GDHZ7vdrrvvvlt//etf9fe//1233nqrUlJSqrRIlNemeZAmjWgvm92uF+bsUNKRM0aXBAAAgMtU6VD+9ddfq2PHjo7P69at03//+1+NHTtW06ZNkyS98847VVchLimysa+eHNVRgb4e+vfH3+nbvSeMLgkAAACXodJPCZ44cUJRUVGOz+vXr1d4eLgeffRRSdKPP/6ozz77rOoqxG8KauipySPb683F32vmZ/uUmXNe/a6NkslkMro0AAAAOKnSI+VFRUVyc/s5y2/dulXdunVzfI6IiGBeeQ3z8rRqwtC2uvaaxlq08aBmr/5BJTaWTAQAAKgtKh3KmzRpop07d0oqHRVPTU1Vp06dHNszMjLk5eVVdRXCKVY3s+699Rr1uzZKG3Ye1ZuLvldBYYnRZQEAAMAJlZ6+0r9/f02fPl2ZmZn68ccf5ePjox49eji2JyUlKTIyskqLhHPMJpOG3NBCQX4emvPlD3pp3g49PCRBDb3djS4NAAAAv6HSI+X333+/7rjjDn333XcymUx68cUX5efnJ0nKzc3VunXr1LVr1yovFM7r2T5c4wfF6Wh6vp6btV0nMs8aXRIAAAB+g8lut9ur6mA2m035+fny9PSU1WqtqsPWiIyMPNlsVXYrnBIc7Kv09NxqO/7BYzl6beEu2e3Sw4Pj1TK8YbWdC6hO1d1XgLqCvgI4x6i+YjabFBTkU/G2qjxRcXGxfH19a10gr6uuauqnKaM6yNvTTS/P36nEA6eMLgkAAAAVqHQo37hxo954440ybXPnzlX79u3Vtm1b/e1vf1NRUVGVFYgrExLgpSdGdVBkYx9N/3SPvvxvqtElAQAA4FcqHcrfe+89HTx40PE5OTlZzz//vEJCQtStWzetWLFCc+fOrdIicWV8vdz12PB2ahcdrHlrf9T8tT/KVnWzlgAAAHCFKh3KDx48qDZt2jg+r1ixQh4eHlq4cKHeffdd9evXT0uWLKnSInHl3K0WPXh7G93UIVyr/5uqGUv3qqiYJRMBAABcQaVDeXZ2tgICAhyfN2/erGuvvVY+PqWT1jt37qy0tLSqqxBVxmw26Y83Xa1hvVpq+/5Tmjr/O+WdY6oRAACA0SodygMCAnTs2DFJUl5enr7//nt17NjRsb24uFglJYzAuiqTyaQ+nSM1bmCsDh3P0fOzE5Wedc7osgAAAOq1Sr88qG3btpo/f75atmypr776SiUlJbr++usd248cOaKQkJAqLRJVr3PrxvL38dAbi3brudmJemRIvJqH+hldFgAAQL1U6ZHyhx9+WDabTX/961+1ePFi3X777WrZsqUkyW63a82aNWrfvn2VF4qqFx3hr8kjO8hqMevFj3Zod/Jpo0sCAAColy7r5UFZWVnasWOHfH191alTJ0d7dna2lixZoi5duqhVq1ZVWmh1q4svD3JWdl6BXv1kt1JP5WlUn2j1aBtmdElAGa7SVwBXR18BnOOKLw+q0jd61mb1OZRL0vnCYk1fskd7DmZqQLco3fGHq2QymYwuC5DkWn0FcGX0FcA5rhjKKz2n/KKUlBStXbtWqamlL6OJiIjQjTfeqMjIyMs9JAzk6e6mhwfHa/YXB7R88xFlZBfo7n6t5Gap0pe+AgAAoAKXFcpfffVVzZw5s9wqKy+//LLuv/9+PfLII1VSHGqWm8WsMbe0UlBDTy35+pCy8gr00B1x8vK87N/dAAAA4IRKD4MuXLhQM2bMUHx8vN566y2tXr1aq1ev1ltvvaW2bdtqxowZWrx4sVPHOnXqlKZOnapRo0apXbt2iomJ0datW52uJTk5WWPHjlW7du3UuXNnTZw4UZmZmZW9JPyCyWTSbdc119j+rfVDapb+NTdRmTnnjS4LAACgTqv0nPJBgwbJarVq7ty5cnMrO4JaXFysESNGqKioyKlgvnXrVo0ePVpRUVEKDAzUzp07NWvWLHXp0uV39z1x4oRuv/12+fn5aeTIkTp79qz+85//KCwsTAsWLJDVaq3MZdX7OeUV2XsoU299+r0aeLhpwp0JCg+peA4UUN1cva8AroK+AjjHFeeUV3qkPDk5Wf369SsXyCXJzc1N/fr1U3JyslPHio2N1ZYtW7R69Wrde++9lapjxowZKigo0OzZszV69GiNGzdOr776qvbt26elS5dW6lioWGzzQE0a0V52u10vzE3UvsP8LQQAAEB1qHQot1qtOnv27CW35+fnOz1K7ePjo4CAgMqWIElavXq1evXqpcaNGzvaunXrpmbNmmnlypWXdUyUF9nYV0+O7qhAX0+9smCXNu85bnRJAAAAdU6lQ3lcXJw+/vhjnT5d/kUzGRkZWrBggRISEqqkuEs5efKkMjIy1KZNm3Lb4uPjlZSUVK3nr28C/Tw1eWR7XR3eUO8uT9LyzYfFSpoAAABVp9LLajz44IMaM2aM+vXrp8GDBzve5vnTTz9p8eLFys/P19SpU6u80F86deqUJCk4OLjctuDgYGVkZKikpEQWi6Va66hPvDytmjC0rd5fmaTFXx1URs55jewdLYuZJRMBAACuVKVDeadOnfTGG2/o2Wef1fvvv19mW9OmTfXiiy+qY8eOVVZgRQoKCiRJ7u7u5bZ5eHhIks6fPy9vb2+nj3mpSffVLTjY15DzXq7JY7pozqokfbL2R+UXlOjxUR3VwIMlE1H9altfAYxCXwGc42p95bLSVK9evXTDDTdoz549SktLk1T68qDY2FgtWLBA/fr104oVK6q00F+6GLwLCwvLbbsY2D09PSt1TFZfcd4tnSLk6WbWnNUH9NjrX+mvdyaooXf5X5CAqlJb+wpQ0+grgHNccfWVyx7iNJvNio+PV3x8fJn2M2fO6NChQ5d7WKeEhIRIktLT08ttS09PV1BQEFNXqlnPdmEK8PHQjGV79Nys7ZowNEGhQc7/zQQAAAB+VisnBDdu3FiBgYHas2dPuW27d+9W69atDaiq/ml7dSNNvKu9CopK9PzsRP2YlmV0SQAAALVSrQjlKSkpSklJKdPWu3dvrVu3TidPnnS0ffvttzp8+LD69u1b0yXWW81D/TRlVAf5NLDq5Xnfafv+U0aXBAAAUOsY/oTe9OnTJcnxwqGlS5cqMTHR8aZOSRozZowkad26dY79xo0bp1WrVmn06NGON3q+9957atWqlQYOHFizF1HPhQR46YlRHfT6ot16e8keDevVUr07RxpdFgAAQK1heCh/7bXXynxetGiRJCksLMwRyisSGhqqOXPm6F//+pemTZsmq9WqG264QZMnT65wVRZUL18vdz02vJ1mfrZP89f9pIycAg27saXMJpPRpQEAALg8k92Jt8D8eunD37J582Zt2rSp1r3Ah9VXqobNZtf8dT9qzfY0dYgJ1p8HXCN3Kw/d4srUxb4CVAf6CuCcWrv6yosvvlipE5oYHa23zGaT7ropWo38PDV/3U/Kzv9ODw+Ol08Dq9GlAQAAuCynQvmsWbOquw7UMb07RyrAz1MzP9un52cnasLQBAX7NzC6LAAAAJfkVCjv3LlzddeBOqhTqxA19HbXG4t267lZ2/XInQlqHupndFkAAAAup1YsiYjaKzrCX0+M6iB3q0UvfrRDu346bXRJAAAALodQjmoXGuStKaM6KDTIW68v2q0N3x01uiQAAACXQihHjWjo46GJd7VT3FVBmrXqgBZtTJYTC/8AAADUC4Ry1BhPdzf9ZXCcrk9oqs+/PaJ3l+9TcYnN6LIAAAAMZ/jLg1C/WMxm/alvjIIaeurTrw4qK69QD90RJy9PfhQBAED9xUg5apzJZNKt3Zrp3gGt9UNqlv41N1GZOeeNLgsAAMAwhHIYplubUP11aIJOZ5/Xc7MTlXoqz+iSAAAADEEoh6FimwVq8sgOkqR/zU3UvsOZBlcEAABQ8wjlMFxEiI+mjOqgQD9PvbJglzbvOW50SQAAADWKUA6XEOjnqckjOig6wl/vLk/SZ5sPs2QiAACoNwjlcBlenm6aMDRBXWMb69OvDurDVQdUYmPJRAAAUPexDh1cipvFrHsHXKNAP099/u0RZeUVaNzAWHm686MKAADqLkbK4XJMJpMG92ih0X1j9P3BDL340U5l5xUYXRYAAEC1IZTDZd3QNkwPD47X8Yx8PTc7Uccz8o0uCQAAoFoQyuHSElo20sS72quwqETPz07UD6lZRpcEAABQ5QjlcHnNQ/30xOiO8vFy19T53+m/+08ZXRIAAECVIpSjVgjxb6ApozqoWRNfzViyR6u3pRhdEgAAQJUhlKPW8Glg1aPD26p9TLDmr/tJH635QTYba5kDAIDaj1COWsXdatEDA9vo5o4RWrM9TW8v2aPCohKjywIAALgihHLUOmazSX+86WoN79VSO35I19T53yn3bKHRZQEAAFw2Qjlqrd6dI/XA7W10+ESunp+dqFNZ54wuCQAA4LIQylGrdWwVokeHt1XeuSI9P2u7Dh3PMbokAACASiOUo9aLjvDXE6M6yN1q0Ysf7dB3P502uiQAAIBKIZSjTggN8taUUR0UGuStNxbt1vqdR40uCQAAwGmEctQZDX08NPGudoq7KkizvzighRuSZbOzZCIAAHB9hHLUKZ7ubvrL4Dj1aNtUK7Yc0bvL96m4xGZ0WQAAAL/JzegCgKpmMZs1uk+Mgvw8tfirg8rKLdD4QXHy8rQaXRoAAECFGClHnWQymTSgWzPdO6C1fkzL1gtzdigz57zRZQEAAFSIUI46rVubUE0YmqDM3PP656ztSjmZa3RJAAAA5RDKUedd0yxQk0d0kMlk0r/m7tDeQ5lGlwQAAFAGoRz1QniIj6aM6qBGDT316ie79M33x40uCQAAwIFQjnoj0M9Tk0Z0UHSEv977PEnLvjkkO0smAgAAF0AoR73i5emmCUMT1DW2iZZ8fUgfrtrPkokAAMBwLImIesfNYta9A1orqKGHlm8+oszcAj14ext5utMdAACAMRgpR71kMpk06PoWGt03RvsOndGLc3cqO6/A6LIAAEA9RShHvXZD2zA9PCROxzPz9c9ZiTp2Ot/okgAAQD1EKEe9F9+ikSbe1V5FxSV6YU6ifkjNMrokAABQzxgaygsLC/Xyyy+re/fuio+P19ChQ/Xtt986te+SJUt06623Ki4uTt27d9c///lP5eczyonL0zzUT1NGd5Svl7umzv9O/91/yuiSAABAPWJoKJ80aZI+/PBD3XbbbZoyZYrMZrP+/Oc/a+fOnb+534cffqiJEycqODhYkyZN0qBBg7Rw4UI9+OCDLHGHyxbs30BPjOqgZqG+envJHn2xLYWfJwAAUCNMdoNSx+7du3XnnXdq8uTJGjNmjCSpoKBAAwYMUEhIiObOnVvhfoWFherWrZtiY2P1wQcfyGQySZLWr1+vcePG6a233tJNN91U6XoyMvJks9XsrQgO9lV6Oq99dzVFxSWa+dk+bT+Qrps6hGv4jVfLbDYZXVa9Rl8BnENfAZxjVF8xm00KCvKpeFsN1+KwatUqWa1W3XnnnY42Dw8PDRkyRImJiTp1quLpAz/++KNyc3PVr18/RyCXpJ49e8rLy0srVqyo9tpRt1ndLBp3exv17hShNYlpmr5kjwqLSowuCwAA1GGGhfKkpCQ1b95c3t7eZdrj4+Nlt9uVlJRU4X6FhYWSSgP8r3l6emrv3r1VXyzqHbPJpOE3Xq0/3ni1dv6Qrpfn71Tu2UKjywIAAHWUYaE8PT1dISEh5dqDg4Ml6ZIj5VFRUTKZTNqxY0eZ9oMHDyozM/OS+wGX4+ZOEXrg9jZKOZmn52cn6tSZs0aXBAAA6iDDXmF4/vx5Wa3Wcu0XR8ALCip+kUtgYKBuueUWLVq0SFdddZVuvPFGnTx5Us8++6ysVusl9/s9l5rfU92Cg30NOS+cd0uwr5qFB+jZ/2zRC3N36O9jr1V0ZIDRZdU79BXAOfQVwDmu1lcMC+Wenp4qKioq134xVFc0PeWiZ555RufPn9cLL7ygF154QZJ02223KTIy0uklFX+NBz3xWxr5WDVpRHu9smCXJr+1SeMGtlHbqxsZXVa9QV8BnENfAZzjig96GhbKg4ODK5xqkp6eLkkVTm25yNfXV2+//baOHTumo0ePqmnTpgoLC9Pw4cMVFRVVbTWjfgsN8taU0R312ie79Mbi3Rp5c7R6tg83uiwAAFAHGDanvFWrVjp06FC5F/7s2rXLsf33NG3aVJ06dVJYWJhycnK0Z88ede3atVrqBSSpobe7Jt7VXvFXBWn26h+0cEOybKxlDgAArpBhobxv374qKirSJ5984mgrLCzU4sWL1b59ezVu3FiSdOzYMSUnJ//u8aZNmyaz2axhw4ZVW82AJHm4WzR+cJxuaNtUK7Yc0buf7VNRsc3osgAAQC1m2PSVhIQE9e3bV1OnTlV6eroiIyP16aef6tixY4554pI0ceJEbdu2TQcOHHC0vf3220pOTlZCQoIsFovWrl2rTZs26ZlnnlFERIQRl4N6xmI2a1SfGAU19NSijQeVlVeg8YPi5OVZ/uFlAACA32NYKJekl156Sa+++qqWLl2q7OxsxcTE6J133lGHDh1+c7+YmBitXbtWa9eulSTFxsZq5syZuv7662uibECSZDKZ1L9rMwX6euo/K5L0wpwd+uudCQpq6Gl0aQAAoJYx2e1MiJVYfQVXJulwpt789Ht5WC36650JimzsWsss1Xb0FcA59BXAOa64+ophc8qBuqR1s0BNHtFBJpNJ/5q7Q3sPZRpdEgAAqEUI5UAVCQ/x0ZRRHdSoYQO9+skubdp93OiSAABALUEoB6pQoJ+nJo1or+gIf/1nRZKWbTokZogBAIDfQygHqpiXp5smDE1QtzZNtGTTIX2wcr+KS1gyEQAAXJqhq68AdZWbxayx/Vsr0M9Tyzcf1pm8Aj0wsI0aeNDlAABAeYyUA9XEZDJp0PVX6U99Y7Tv0Bm9+NEOZeUVGF0WAABwQYRyoJr1aBumh4fE6WTmOT03K1HHTucbXRIAAHAxhHKgBsS3aKSJI9qpqMSm52cn6kDKGaNLAgAALoRQDtSQZk38NGVUB/l5u2vax99pW9JJo0sCAAAuglAO1KBg/wZ6YlQHNQ/104yle7VqawpLJgIAAEI5UNN8Glj16PC26tgqRAvW/6SPvvxRNhvBHACA+oz12QADWN0sGjcwVgt8PbT6v6nKzD2v+26LlYfVYnRpAADAAIyUAwYxm0wafuPV+uNNV+u7H09r6rydyjlbaHRZAADAAIRywGA3d4zQg3e0UcqpPD0/O1Enz5w1uiQAAFDDCOWAC+gQE6LHhrfT2fPFem5WopKPZRtdEgAAqEGEcsBFtAxvqCdGdVADD4te/mindv6QbnRJAACghhDKARfSJNBLU0Z1VFiwt9789HutTUwzuiQAAFADCOWAi/Hzdtfjf2yvhBaNNPfLH/TJ+p9kYy1zAADqNEI54II83C16aFAb9WwXppVbU/TOsr0qKrYZXRYAAKgmrFMOuCiL2ayRvaMV6OehRRsPKjuvUOMHx8nb02p0aQAAoIoxUg64MJPJpP5dm+m+W6/RT0ez9cKcHTqdfc7osgAAQBUjlAO1wLWxTfQ/w9rqTG6BnpudqJSTuUaXBAAAqhChHKglWkcFaPLI9rKYTXph7g7tOZRhdEkAAKCKEMqBWiQ82EdTRnVUcMMGeu2T3dq0+7jRJQEAgCpAKAdqmQBfD00e2V6tIv31nxVJWrrpkOwsmQgAQK3G6itOKioqVG5uloqLC2WzlVTJMU+dMstmY5m7usRicZOPj78aNPCu1vM08HDTI3cm6MOV+7V00yFl5JzX6D4xcrPwezYAALURodwJ587lKzf3jHx8GsrDI1Bms0Umk+mKj+vmZlYxa0/XGXa7XUVFhcrKSpekag/mbhaz7unfWkENPbXsm8PKyi3QA7e3UQMPujUAALUNw2pOyMvLlr9/I3l5+cpicauSQI66x2Qyyd3dQ/7+wcrLy6qxc97+h6s05pZW2nf4jF78aIey8gpq5NwAAKDqEMqdUFJSJKvVw+gyUEtYre4qKSmu0XNen9BUDw+J18nMc3pu1nYdPZ1fo+cHAABXhlDuJEbH4SyjflbiWwRp0oj2Ki6x64XZiTqQcsaQOgAAQOURyoE6JKqJr6aM6qCGPu6a9vF32rrvpNElAQAAJxDKUa3Gj79P48ffV+P71meN/Bto8sgOuirUT/+3bK9Wbj3CkokAALg4lmmop7p37+jU9z75ZJlCQ5tWczWoaj4NrPrb8LZ6d3mSPlmfrIzs87rrpmiZzUzDAgDAFRHK66mnnnqmzOcFC+bp5Mnj+stf/qdMu79/wBWd55VX3jJkX0hWN4vuHxirQD8PfbEtVWdyC3TfbbHysFqMLg0AAEN8u/eEFm9MVmZOgQL9PDSoRwt1jW1idFmSCOX1Vp8+/cp83rBhrbKzs8q1/9r58+fl6enp9HmsVutl1Xel+6KU2WTSsF5XK8jPU/PW/KiX5+3Uw0Pi5eflbnRpAKqQKwcNoKbZ7XbZJcku2WWX3S7Z7dLWfSc0Z/UPKrzwjpiMnAJ9uHK/JLlEfyGU45LGj79PeXl5evzxJ/TGG6/owIH9GjFitMaOvV9ff71By5Z9qh9+OKCcnGwFB4eoX79bNWrU3bJYLGWOIUlvvvmOJGnHju16+OFxeu65l3To0EEtWbJIOTnZiotL0GOPPaHw8Igq2VeSFi1aoPnz5yoj47RatGih8eMnaObMt8scs764qWOEAnw99c5ne/X87ERNGJqgxgFeRpcFoAp8u/eEPly5v1zQsNvtuvaaJmVCiX7x7+XadaHd/vO/y162XZJspTtfaP85/NgubP/lMcq1Xzhnhce50H7x3399fLu9fI0VtV9o/sVxLhz7V9f8y1p+eU7Z7bL9XrvjOBfvyaXuVfl7rgruf7n2cve//Hcdx67onBXdk1/fK8c5f+e7F87/6z+rCtsN+lkod65KKCy2afHGZEJ5ffbt3hNa/NVBZWSfV5ALj2pkZZ3R449PUO/efdW3b381blxa44oVy9WggZeGDRshL68GSkzcrnffnaH8/Hw99NAjv3vcDz98T2azRXfdNVq5uTmaN2+2YVz/BgAAIABJREFUnn76Sc2c+WGV7Pvppwv1yisvqW3b9ho27I86fvy4Jk9+VL6+vgoODrn8G1KLdYgJ1mM+7fT6wt16blaiHhkSrxb/396dh0VZr30A/84AMywz7MNAbAoqpCKClZGaKWJobhlquWTqsdwo9e2ctve8x3PK8lJzyYVMTic1yxJFcEnFpVNpV57E3JHjCsQyLAIzrDPMvH8AAxOjgIrPAN/PdZXym2e5Z2Tgnt9zP/fP20nosIjoHhgMBhRrqpGRp8aXh68YE/J61To94vddRvy+ywJFSAAgqvufuK5VrkhU2za3frz+7yIRIIII9R11RSJR3Vhdm91GfzcZb3JMEcRmjgHUjf/xnOZiqdu3/h4k87GYjpmLu0Wx1B9H1Hjc9DWByPT1udt43SMmz03U6Ji7f7hu9t+psNQyFt1jUi6AO81qAJZx+aSxgoJ8vP32XzFq1FiT8SVLPoBU2lDGMm5cDFas+BCJiTsxe/ZcSCR3L4/Q6XT4/PMtsLau/RZ0dHTC2rUrcf36VQQEdLuvfbVaLeLj49CrVwjWrNlo3K5bt+5YunRJp03KAaCbtxPendYPq7/9DSu+PoPXxvRCWA+F0GER0V3oDQaoblcgI0+NW3lqZORpkJmnRmm5ttl9xw3s2iShA+qSRDPJ1R2Tv8bjwF0TzYbxRklRs4mYaSzmE7q7JGh/iKNxInbXceNxmnsupklzw+t3p9eV65tYon//9rvZBNzN0TIWiGRSfh9OnM/BT+dyWr3ftewS6GpML7BU6/T414HL+OG37FYfb2AfLwwI8Wr1fi1ha2uL6Ojnmow3TsjLy8tQXa1FaGgYkpJ249atm+jevcddj/vcc2OMyTIAhIb2BQBkZ//ebFLe3L5paZdQUlKCefOeN9kuKioan3yy6q7H7gw8Xe3x3rTHsDbhHNbvPo/JUT0Q2c9H6LCICIBWp0d2QVld8q1GhkqDTJUGVdU1AAArsQje7g7oE+gOP6UMfko5NiVfxG21+URjzMCuD/spEFms8YMDTSZFAUBiLcb4wYECRtWASbkA/piQNzcuJIXCwySxrXf9+jVs3hyH1NT/oKzMdEn3sjJNs8etL4OpJ5c7AgDUavV975ubW/tB6Y815tbW1vDyapsPL+2No4MEf3kpDJuSL2J7SjoKSysR80ygcVaIiNpeRZWuNvHO0yBDVftndkEZauoKl6USK/h5yDAwxKs2AfeQw1vhAGsr0yVGYp6x7ESDyFLUVyNY6k3RTMrvw4CQe5uh/vPGE3e8fPLWlPAHEdoD03hGvJ5arUZs7Kuwt5dh1qw58Pb2gUQiQXp6GuLi1kGv15s5kimx2HxbvpYscnM/+1IDqcQKC8aHYHtKOg7+koGi0krMeq4nbKy5phjRg1asqWpIwOv+VBVXGB93dJDATylDn0A3+HrI4K+UQ+Fi16IPypaeaBBZkohenojo5QmFQo78/OYnAh8mQZPy6upqrF27FklJSSgtLUVwcDAWLVqEiIiIZvc9efIk4uLikJ6eDr1ej4CAAEyfPh0jR969pZ8lsPTLJ805c+Y0SkpKsHTpCvTt2/AhIien9aU3bcHTs/aDUlZWJkJDw4zjOp0OOTk5CAy8e3lMZyIWizB1eA+4Odki4ftrKNZUI/aFEDjYsh0l0b3QGwzIv12BDJXGpAa8tKzauI2Hsx38lDIM6OMF/7oSFGfZ/dW0WnKiQUQtI2hS/vbbb+Pw4cN4+eWX4e/vj8TERMyePRvbtm1DWFjYHfc7fvw45s6di7CwMMTGxgIA9u/fj0WLFqGsrAwTJkx4WE/hnhhnNdpB9xVzxOLamdTGM9NarRaJiTuFCslEcHBPODk5ITk5Ec8+O9JYfpOSchBqdanA0VkekUiEkU/6w1UuxT/3Xza2THR3shM6NCKLpqtpXP9dm4RnqjSobFT//Yi7A0K6usJPKYefUgZfDznsbXmRmoiaEuwnw7lz57B//3688847eOWVVwAA48aNw6hRo7By5Ups3779jvtu374dCoUCW7ZsMXb5mDhxIiIjI5GUlGTxSTlQm5gPCn0EOl3zpR6WJiSkD+RyRyxdugQxMZMgEolw6NABWEr1iI2NDWbOfBWrV6/AwoXzMGRIJHJycvDdd3vh7e3DO+Lv4MlennCWSbFu93ks3XoaCyeEwt9TLnRYRBahokqHTJXGpATl98b13zZW8PWQ4anenvBTyuGvlOMRdweWgxFRiwmWlB88eBA2NjYmCbRUKkVMTAxWr14NlUoFDw/zres0Gg2cnJxM2u5JJBI4OTlBKrWMtjYdmZOTM5YvX43169dg8+Y4yOWOGD58BB577AksXrxA6PAAAC+8MAkGgwE7dmzHhg1rERjYHcuWrcKaNSshkfB75E6C/V3w7tRwrN55Fsu+SsX8cb3RO8BN6LCIHqqSsuq65FuNW3UJuOp2Q/233N4Gfko5hge4wl8ph59SDo8W1n8TEd2JyCDQ3XEzZsxAQUEB9u7dazL+888/45VXXsFnn32GwYMHm9131apV2LRpE+bOnYvx48cDAHbv3o34+Hhs2rQJAwYMaHU8hYUa6PXmX4rc3Fvw9PRv9TGbY20tbpcz5e2VXq/HqFFRGDx4CN5663/b9Fxt9T3zsNxWV2HNzrP4Pb8M06ODMCj0EUHjYZ0stQWDwYD84gpk5GkaSlBUapRoGuq/3Z1s6xJvWV0JihzOMonFXnHje4WoZYR6r4jFIri5ycw+JthMeX5+PpRKZZNxhaJ2IROVSnXHfefMmYOMjAx8+umniIurXTbd3t4eGzduvKeEnDqeqqqqJldNDh7cj9LSEoSF9RMoqvbDRS7F21PCsTHxPP71XRoKSysxdmBXi01EiJpTX/9t7H6i0iBTpUZFVW39t1gkwiPu9ujVxbWu/EQGXw8Z7HnTMxE9JIIl5ZWVlbCxafrDrj6Rqqq685KnEokEXbp0QXR0NKKiolBTU4Nvv/0WCxcuxBdffIE+ffq0Op47fWoBAJVKDOs2qgtsq+N2dqmp57Bhw1oMGRIJJycnXLmShr17kxAY2A1RUcPb/HUXi8VQKNp/PfYH8wZi/c7fkHziJsqr9Zg/IbRJj+SHpSO8nvRwVFTpcCO7BNd/r/svuwS3ctTQ1dRemZRKrNDVyxFD+vkiwNsZAd6O8Pd0hMTGfLvV9obvFaKWsbT3imBJua2tLbTapksE1yfjd6sNf//993H+/HkkJCQYO4GMGDECo0aNwocffogdO3a0Op67la/o9fo2KTNh+UrbUSq94OamwLff7kBpaQkcHZ0QHf0c5sxZAJHIqs1fd71e32EuIU8e2g0OEiskn7iJnAIN5o3rDTvpw/3RwUvydCel9fXfxhaEGqiKylH/01xmZwN/pQxRj/nAV1nb/1vpYg+x2PSqT0lx+cMPvg3wvULUMixfaUShUJgtUcnPzweAO97kWV1djYSEBLz22mvGhByo7bgxaNAgfP3119DpdGZXoaTOw9vbB8uXrxY6jA5BJBJh3KAAuDraYuvBK1i2PRULJ4TCRc4bZunhMRgMyC+pRGajmy8z8tQo/kP9t59SjoieSmMLQhe5lGVXRNQuCJa5BgcHY9u2bSgrK4ODg4Nx/OzZs8bHzSkuLoZOp0NNTU2Tx3Q6HXQ6HVd2JGoDT4c+Ahe5FBv3XMDSbb9i0YRQeCvuXPZFdK90NXrkFpab9P/OUGlQUaUDUFv/7eVuj0f9XYw3X/opZVz0iojaNcGS8ujoaHz++efYuXOnsU95dXU1du/ejfDwcONNoNnZ2aioqEBgYO1ql25ubnB0dERKSgoWLFhgrEsvKyvD8ePH0aNHD7O16kR0/0IC3PD25HCs2XkWH36ZitjxIQj2dxE6LGrHqqprkJmvMWlB+Ht+mbH+W2Itho+HDP17KuFXV37i7e7QYeq/iYjqCZaUh4aGIjo6GitXrkR+fj78/PyQmJiI7OxsfPTRR8bt3nrrLZw6dQpXrlwBAFhZWWHmzJlYs2YNJk2ahDFjxkCv1yMhIQG5ubl46623hHpKRJ2Cv6cc773cD6u/PYtV3/6Gmc89iid7to/VaElY6vJq48x3/Sx4XqP6bwdba/gp5RjWz8fYgtDTtWn9NxFRRyRo4fXy5cuxZs0aJCUloaSkBEFBQfjss8/Qr9/dW9bNnTsXPj4+2Lp1KzZs2IDq6moEBQVh/fr1iIqKekjRE3Ve7k52eHdaP6zbdR6fJV9CUWkVRvT3Y+0uAait/y4sqTSp/c5QaXBb3dBVy81RCj+l3GQGnPXfRNSZCbZ4kKXh4kH0ILX3xYNaSqvT45/7L+HUZRWGhHljSlSPNpnVZEcJy1Wj1yOnsNxk+flMlQZllbX13yIR4OXmUDvz7VHX/1sph8yOZYZtge8VopZh9xUi6lBsrMV4dUwvuDra4uAvGbitrsJrY3tBynrfDqlKW4MslcbYfjAjT42s/DJo6yYXbKzF8FHI8HiwB3zrbr70Ucj4/UBE1AJMyonovohFIkwc0g1ujrb4KiUdy786gzdi+sDRQSJ0aHQfNBVak9nvW3lq5BaVo/7aqoOtNXw9ZBgS5m1cht7TzR5WYi6IRkR0L5iUE9EDEdnPBy5yKTYlX8TSbb9i8cS+ULraCx0WNcNgMKCotMrk5ssMlRpFpQ313y5yKfyVcjwe7GFsP+jmaMv6byKiB4hJOT0QBw7sxYcf/h07dybDy+sRAEBMzGiEhfXDe+8tafW+9ys19Ve8/vocfPLJpwgPf+yBHJOaF95Dgb+8FIa1CeewdNtpvB7TB928nYQOi+ro9QbkFJU33HxZNwturP8G4Olmj+4+zsbuJ34eMsjtedWDiKitMSnvpP7yl0VITf0P9u5NgZ2dndltFi9egIsXzyM5+TCkUstcvfHIkUMoKirExImThQ6F6gR6O9W2TPzmLFZ8fQavju6FfkEKocPqdKq1NcjKL/tD/28Nquvqv62txPBROKBfkAf86xJwH4UMUgnrv4mIhMCkvJOKinoWJ0/+iJ9++jeioqKbPH77dhFOn/4Phg8fcc8J+Vdf7YK4jetLjx49jP/+N71JUt63bziOHj3BhaQEonSxx7sv98MnCeewMfE8Jkf1QGQ/H6HD6rA0FdqG5edVtTPgOYVlxvpvO6k1/JUyPBPmbeyC4ulmD2sr1n8TEVkKJuWd1KBBz8DOzh5Hjhwym5QfO3YENTU1GD686WMtJZEId8lbLBZb7Ox+Z+FoL8GfXwrDZ8kXsT0lHYUllYgZEggx65DvmcFgwG11leny83kaFJZWGrdxkUvh5yFDvx4KYwmKuxPrv4mILB2T8k7K1tYWgwYNxvHjR1BaWgpHR0eTx48cOQQ3Nzf4+vpj5cplOH36FPLy8mBra4vw8Mcwf/4bzdZ/m6spv379GtasWYELF87DyckJY8eOh7t709KGH3/8HsnJiUhPv4LS0hIoFB4YOXI0pk2bASur2svrCxa8it9+SwUADBxYWzfu6emFhIS9d6wpP3r0ML788gvcunUT9vYOGDBgEObOfR3Ozs7GbRYseBUajQb/93//wKpVy3H58kXI5Y6YMOFFTJkyvXUvdCcntbHC/OdD8NWRdBw8lYEidSVmPfcobKxZItEcvd6A3KJG/b/rZsA1FVoAtfXfSld7BHo7Yki4tzEBd2T9NxFRu8SkXCCnclOx9/pBFFUWw0XqjDGB0XjCM/yhxhAVFY3Dh7/D998fxZgxzxvHc3NzcOHCOcTEvIjLly/iwoVzGDbsWSgUHsjJycaePbsQG/savvxyJ2xtbVt8vsLCArz++hzo9XpMnTodtrZ2SE5ONDujfeDAPtjZ2WPSpCmwt7fD6dO/Ij7+U5SVlWH+/DcAANOnz0RFRQXy8nIQG7sYAGBnd+duH/U3lPbqFYK5c1+HSpWHXbu+weXLF7F581aTOEpLS/A///M6hgyJRGTkcBw/fgRxcesQENANEREDWvycqXahhClRPeDmZIudx6+hWFONBeNDuHhMI1pdbf33rTw1MusX4MnXoFpbX/8tgrdChrDu7vBTyuGvlMPHwwG2Ev4IJyLqKPgTXQCnclPxVdouaPW1M163q4rxVdouAHioifnjj/eHs7MLjhw5ZJKUHzlyCAaDAVFRzyIwsBuGDBlmst+AAU9jzpwZ+P77o4iOfq7F59u+fQtKSooRH78NQUHBAIARI0bhpZeeb7LtkiUfQCptSPjHjYvBihUfIjFxJ2bPnguJRILHH38Su3fvRElJMZ59duRdz63T6RAXtw7duvXAunWbjKU1QUHBWLLkPezdm4iYmBeN26tUefjb3z4wlvaMGjUWMTGjsH9/EpPyeyASiTCivz9c5FJ8vv8yPvryNBZNDIW7k/mbjDuyskotMvI0JjXgOQXl0NcVgNtJreDrIcfToY/U9f+Ww4v130REHR6T8vvwS85p/Jzzn1bvd6MkAzqDzmRMq9di++UEnMw+1erjRXg9jv5e/Vq9n7W1NYYOHYY9e3ahoKAA7u7uAIAjRw7Dx8cXPXv2Ntlep9OhrEwDHx9fyGRypKentSop//nnEwgJCTUm5ADg4uKCqKgRSEzcabJt44S8vLwM1dVahIaGISlpN27duonu3Xu06rmmpV3C7dtFxoS+3tChUdiwYS1OnjxhkpTLZDIMG/as8WsbGxs8+mgvZGf/3qrzkqkne3rCRSbFul3nsXTraSycEAp/T7nQYbWJ+vrvhtUva/8sKGmo/3aSSeCvlNfOgHvI4ecph4L130REnRKTcgH8MSFvbrwtRUVFY/funTh27DAmTpyMmzdv4OrVdMyYMRsAUFVViW3bvsCBA3uRn6+Cob6dAwCNRtOqc+Xl5SIkJLTJuJ+ff5Ox69evYfPmOKSm/gdlZWUmj5WVte68QG1JjrlzicVi+Pj4Ii8vx2Tcw0PZJDGSyx1x7drVVp+bTAX5ueCdqeFYvfMslm1PxbzneyMkwE3osO6LXm9A3u3yRjdf1s6C19d/A4DSxQ5dvRwxuG/tDLivUg4nrnpKRER1mJTfh/5e/e5phvp/T3yI21XFTcZdpM5YGD7nQYTWYiEhofDy8kZKykFMnDgZKSkHAcBYtrF69QocOLAXEya8hN69QyCTyQCIsGTJuyYJ+oOkVqsRG/sq7O1lmDVrDry9fSCRSJCenoa4uHXQ6/Vtct7GxGLzNyK21XPubLwVMrw37TGs3XkWa3eew8vRQXg69MEsHNXWtDo9fi/QICNPU9cFRY0sVRmqtDUAACuxCN4KB/Tt7m5cft5HIYOdlD9uiYjozvhbQgBjAqNNasoBwEZsgzGB995+8H4MGzYc27b9C1lZmTh69DCCgh41zijX143Hxi4ybl9VVdXqWXIAUCo9kZWV2WQ8I+OWyddnzpxGSUkJli5dgb59G2rsc3KyzRy1ZZf5PT29jOdqfEyDwYCsrEx07RrYouPQg+Mil+KtKeHYuOcCvvguDUWllRg7sKtFlW6UV+qQqaqr/a5LwHMKy1Gjr/1wZiuxgp+HDIP6eBmXn3/E3YH130RE1GpMygVQfzOn0N1X6g0fPgLbtv0L69evRlZWpkkCbm7GeNeub1BTU9Pq80REDMDOnTtw5Uqasa789u3bSEn5zmS7+gWHGs9Ka7XaJnXnAGBnZ9eiDwjBwT3h4uKKPXsSMGLEKOOiQsePH0V+vgpTprzc6udD989Oao03Yvpg68ErSD5xE4WllZgeHfzQk1qDwYBiTbXp8vMqNfKLG9V/O0jgp5QjtJu7MQFXONux7zoRET0QTMoF8oRnOJ7yeQw6XduXYjSna9cAdOvWAz/99APEYjEiIxtucHzqqYE4dOgAHBxk6NKlKy5ePI9ffz0FJyenVp9n8uTpOHToABYvno+YmBchldoiOTkRSqUXNJr/GrcLCekDudwRS5cuQUzMJIhEIhw6dADmKkeCgoJx+PB3WLduFYKDe8LOzh4DBz7dZDtra2vMnRuLDz/8O2JjX8OwYcOhUuUhIeEbBAQEYvToph1g6OGwthJjxshguDnZIumnGyhWV2He8yFtVu6hNxigul1RV/fd0IKwtLzhypWHix38PR0xqM8jdS0IZXCScTEqIiJqO0zKCQAwfHg0rl5NR1hYP2MXFgB44403IRaLkZLyHaqqqhESEoo1azZg8eLYVp/D3d0dn3yyCatXL8e2bV+YLB60bNn7xu2cnJyxfPlqrF+/Bps3x0Eud8Tw4SPw2GNPYPHiBSbHHDv2BaSnp+HAgX345puv4OnpZTYpB4CRI0dDIpFg+/Yt2LBhLRwcHBAVFY05c2K5+qfARCIRxg7sCle5FFsOXsGy7alYOCEULvL7+3fR6vTILmjo/31LpUamSoOq6kb13+4OCAl0M/b/9vVg/TcRET18IgPvXAMAFBZqoNebfylyc2/B07Nph5D7ZW0ttoiZcnrw2up7pjO4cL0QG/ZcgIOtNYaGeeP4md9RVFoFV0cpxg8OREQvT7P7VVTpaktPGrUgzC4oM9Z/SyVW8PWQwd9Dblz98hF3B9hYs/6bOg6FQo78fLXQYRBZPKHeK2KxCG5uMrOPcTqIiCxK7wA3vD05HMu/TkXCv68bxwtLq7DluzQAwKP+LibtBzPyNFAVVxi3dbS3gZ9SjpAAN/gpZfBXyqFwYf03ERFZLiblRGRx/D3lkNpYoaLK9Ibiap0e8fsumdxfoHC2hZ9SjgF9vOCvlMHXQw5nmcSiurgQERE1h0k5EVmkYk212XGDAXgxsrsxAbe35Y8xIiJq//jbjIgskpujFIWlVWbHhz/uK0BEREREbYd3OBGRRRo/OBCSP9yEKbEWY/xgLvREREQdD2fKicgi1XdZ2f3vay3qvkJERNSeMSlvIYPBwBvHqEXYZfTBiejliYhenmzzRkREHR7LV1rAysoGWm3T2lYic7TaalhZ8fMuERERtRyT8haQyZxQXFyAsjI1amp0nAklswwGA6qrq1BcnA+ZzFnocIiIiKgd4XReC9jZOcDa2gYaTTHKykqg19c0v1MLiMVi6PVc0bMjsbKyhlzuAjs7B6FDISIionaESXkL2dhI4OLi8UCPyTpZIiIiIgJYvkJEREREJDgm5UREREREAmNSTkREREQkMCblREREREQCY1JORERERCQwdl+pIxYLs1qnUOclam/4XiFqGb5XiFpGiPfK3c4pMnAlHCIiIiIiQbF8hYiIiIhIYEzKiYiIiIgExqSciIiIiEhgTMqJiIiIiATGpJyIiIiISGBMyomIiIiIBMaknIiIiIhIYEzKiYiIiIgExqSciIiIiEhgTMqJiIiIiARmLXQAnY1KpcLWrVtx9uxZXLhwAeXl5di6dSv69+8vdGhEFuPcuXNITEzEL7/8guzsbDg7OyMsLAwLFy6Ev7+/0OERWYzz58/j008/xaVLl1BYWAi5XI7g4GDMnz8f4eHhQodHZNE2b96MlStXIjg4GElJSUKHw6T8Ybtx4wY2b94Mf39/BAUF4cyZM0KHRGRx4uPjkZqaiujoaAQFBSE/Px/bt2/HuHHjkJCQgMDAQKFDJLIImZmZqKmpwYQJE6BQKKBWq7F3715MnToVmzdvxoABA4QOkcgi5efnIy4uDvb29kKHYiQyGAwGoYPoTDQaDbRaLVxcXHDkyBHMnz+fM+VEf5CamorevXtDIpEYx27evInRo0fjueeew7JlywSMjsiyVVRUYNiwYejduzc2bdokdDhEFuntt99GdnY2DAYDSktLLWKmnDXlD5lMJoOLi4vQYRBZtPDwcJOEHAC6dOmC7t2749q1awJFRdQ+2NnZwdXVFaWlpUKHQmSRzp07h+TkZLzzzjtCh2KCSTkRtQsGgwEFBQX8UEtkhkajQVFREa5fv45Vq1YhPT0dERERQodFZHEMBgPef/99jBs3Do8++qjQ4ZhgTTkRtQvJycnIy8vDokWLhA6FyOK8++67OHToEADAxsYGL774IubMmSNwVESWZ8+ePbh69So2bNggdChNMCknIot37do1/OMf/0C/fv0wduxYocMhsjjz58/HpEmTkJubi6SkJFRXV0Or1TYpAyPqzDQaDT7++GO8+uqr8PDwEDqcJli+QkQWLT8/H6+99hqcnJywdu1aiMX8sUX0R0FBQRgwYABeeOEF/POf/8TFixctrl6WSGhxcXGwsbHBjBkzhA7FLP52IyKLpVarMXv2bKjVasTHx0OhUAgdEpHFs7GxQWRkJA4fPozKykqhwyGyCCqVClu2bMHkyZNRUFCArKwsZGVloaqqClqtFllZWSgpKRE0RpavEJFFqqqqwpw5c3Dz5k188cUXCAgIEDokonajsrISBoMBZWVlsLW1FTocIsEVFhZCq9Vi5cqVWLlyZZPHIyMjMXv2bLz55psCRFeLSTkRWZyamhosXLgQv/32GzZu3Ii+ffsKHRKRRSoqKoKrq6vJmEajwaFDh+Dl5QU3NzeBIiOyLD4+PmZv7lyzZg3Ky8vx7rvvokuXLg8/sEaYlAtg48aNAGDst5yUlITTp0/D0dERU6dOFTI0IouwbNkyHDt2DEOGDEFxcbHJog4ODg4YNmyYgNERWY6FCxdCKpUiLCwMCoUCOTk52L17N3Jzc7Fq1SqhwyOyGHK53Ozvji1btsDKysoifq9wRU8BBAUFmR339vbGsWPHHnI0RJZn2rRpOHXqlNnH+D4hapCQkICkpCRcvXoVpaWlkMvl6Nu3L2bOnIknnnhC6PCILN60adMsZkVPJuVERERERAJj9xUiIiIiIoExKSciIiIiEhiTciIiIiIigTEpJyIiIiISGJNyIiIiIiKBMSknIiIiIhIYk3IiIiIiIoExKSciIsFMmzYNQ4cOFToMIiLBWQsdABERPVi//PILXn755Ts+bmVlhUuXLj3EiIiIqDlMyomIOqhRo0bh6aefbjIuFvMiKRGRpWFSTkTUQfXs2RPgNFRBAAAEaklEQVRjx44VOgwiImoBTpcQEXVSWVlZCAoKwrp167Bv3z6MHj0aISEheOaZZ7Bu3TrodLom+6SlpWH+/Pno378/QkJCMHLkSGzevBk1NTVNts3Pz8cHH3yAyMhI9O7dGxEREZgxYwZOnDjRZNu8vDwsXrwYjz/+OEJDQzFr1izcuHGjTZ43EZEl4kw5EVEHVVFRgaKioibjEokEMpnM+PWxY8eQmZmJKVOmwN3dHceOHcP69euRnZ2Njz76yLjd+fPnMW3aNFhbWxu3PX78OFauXIm0tDR8/PHHxm2zsrLw0ksvobCwEGPHjkXv3r1RUVGBs2fP4uTJkxgwYIBx2/LyckydOhWhoaFYtGgRsrKysHXrVsybNw/79u2DlZVVG71CRESWg0k5EVEHtW7dOqxbt67J+DPPPINNmzYZv05LS0NCQgJ69eoFAJg6dSoWLFiA3bt3Y9KkSejbty8AYOnSpaiursaOHTsQHBxs3HbhwoXYt28fYmJiEBERAQD4+9//DpVKhfj4eAwaNMjk/Hq93uTr27dvY9asWZg9e7ZxzNXVFStWrMDJkyeb7E9E1BExKSci6qAmTZqE6OjoJuOurq4mXz/11FPGhBwARCIR/vSnP+HIkSNISUlB3759UVhYiDNnziAqKsqYkNdvO3fuXBw8eBApKSmIiIhAcXExfvzxRwwaNMhsQv3HG03FYnGTbjFPPvkkAODWrVtMyomoU2BSTkTUQfn7++Opp55qdrvAwMAmY926dQMAZGZmAqgtR2k83lhAQADEYrFx24yMDBgMBvTs2bNFcXp4eEAqlZqMOTs7AwCKi4tbdAwiovaON3oSEZGg7lYzbjAYHmIkRETCYVJORNTJXbt2rcnY1atXAQC+vr4AAB8fH5Pxxq5fvw69Xm/c1s/PDyKRCJcvX26rkImIOhwm5UREndzJkydx8eJF49cGgwHx8fEAgGHDhgEA3NzcEBYWhuPHjyM9Pd1k288++wwAEBUVBaC29OTpp5/GDz/8gJMnTzY5H2e/iYiaYk05EVEHdenSJSQlJZl9rD7ZBoDg4GBMnz4dU6ZMgUKhwNGjR3Hy5EmMHTsWYWFhxu3ee+89TJs2DVOmTMHkyZOhUChw/Phx/PTTTxg1apSx8woA/PWvf8WlS5cwe/ZsjBs3Dr169UJVVRXOnj0Lb29v/PnPf267J05E1A4xKSci6qD27duHffv2mX3s8OHDxlruoUOHomvXrti0aRNu3LgBNzc3zJs3D/PmzTPZJyQkBDt27MAnn3yCr7/+GuXl5fD19cWbb76JmTNnmmzr6+uLXbt2YcOGDfjhhx+QlJQER0dHBAcHY9KkSW3zhImI2jGRgdcRiYg6paysLERGRmLBggWIjY0VOhwiok6NNeVERERERAJjUk5EREREJDAm5UREREREAmNNORERERGRwDhTTkREREQkMCblREREREQCY1JORERERCQwJuVERERERAJjUk5EREREJDAm5UREREREAvt/3YwahYMGHekAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yHQTUhYthQZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4303b2e5-1710-4219-9367-c71502fa64c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Number of training sentences: 2,500\n",
            "\n",
            "Max sentence length:  6643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,   713,    16,    10, 10262,   205,    78,  5808,     4,  1437,\n",
            "           616,     5,  2236,  4428,   101,    14,     9,    10,   239,   334,\n",
            "          1816,     6,     5, 24587,     6,  9817, 21930,     6,    16,    10,\n",
            "           706,    12,   180,    12,   279,   784, 32916,    54,    21,   553,\n",
            "             7,  3116,     5,  6168,   405, 16705,     9,    41,   793,  1692,\n",
            "           334,  1441,    54,  2021,  4260,     6,  2875,   957,     4,  1437,\n",
            "           152,  3685,  5699,     2]])\n",
            "2500\n",
            "2500\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(r'/content/drive/MyDrive/NLP_Fall2022/Group Project/DatasetSplit/test_split.xlsx', usecols=['label','review'])\n",
        "\n",
        " \n",
        "df = df.reset_index()\n",
        "print(\" \")\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "#  +\" \"+df['sentence2']\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(df['review'][val], add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for val in range(len(df['review'])):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        df['review'][val],                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "labelToList = list(df['label'])\n",
        "\n",
        "labels=[]\n",
        "\n",
        "for labelVal in labelToList:\n",
        "  labels.append(labelVal)\n",
        "print(input_ids[0])\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "print(len(input_ids))\n",
        "print(len(attention_masks))\n",
        "print(len(labels))\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  pred_labels = np.argmax(logits, axis=1)\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.extend(pred_labels.tolist())\n",
        "  true_labels.extend(label_ids.tolist())\n",
        "\n",
        "print('    DONE.')\n",
        "\n",
        "\n",
        "true_labels_str=[]\n",
        "\n",
        "for labelVal in true_labels:\n",
        "  true_labels_str.append(labelVal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_labels_str=[]\n",
        "\n",
        "for labelVal in predictions:\n",
        "  predicted_labels_str.append(labelVal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_Mpsu5yri2LN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d126ab-b3a5-4da5-f504-b0122dc05e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.263     0.227     0.244        22\n",
            "           1      0.913     0.808     0.857        26\n",
            "           2      0.739     0.810     0.773        21\n",
            "           3      0.741     0.833     0.784        24\n",
            "           4      0.542     0.464     0.500        28\n",
            "           5      0.389     0.292     0.333        24\n",
            "           6      0.647     0.393     0.489        28\n",
            "           7      0.357     0.800     0.494        25\n",
            "           8      0.727     0.727     0.727        22\n",
            "           9      0.414     0.632     0.500        19\n",
            "          10      0.645     0.690     0.667        29\n",
            "          11      0.727     0.640     0.681        25\n",
            "          12      0.857     0.960     0.906        25\n",
            "          13      0.842     0.727     0.780        22\n",
            "          14      0.692     0.429     0.529        21\n",
            "          15      0.792     0.864     0.826        22\n",
            "          16      0.333     0.130     0.188        23\n",
            "          17      0.789     0.625     0.698        24\n",
            "          18      0.889     0.857     0.873        28\n",
            "          19      0.714     0.417     0.526        24\n",
            "          20      0.773     0.548     0.642        31\n",
            "          21      0.524     0.440     0.478        25\n",
            "          22      0.500     0.300     0.375        20\n",
            "          23      0.625     0.345     0.444        29\n",
            "          24      0.750     0.818     0.783        22\n",
            "          25      0.700     0.636     0.667        22\n",
            "          26      0.657     0.793     0.719        29\n",
            "          27      0.793     0.920     0.852        25\n",
            "          28      0.567     0.708     0.630        24\n",
            "          29      0.550     0.314     0.400        35\n",
            "          30      0.933     0.667     0.778        21\n",
            "          31      0.853     0.967     0.906        30\n",
            "          32      0.745     0.897     0.814        39\n",
            "          33      0.481     0.542     0.510        24\n",
            "          34      0.808     0.750     0.778        28\n",
            "          35      0.885     0.767     0.821        30\n",
            "          36      0.900     0.783     0.837        23\n",
            "          37      0.743     0.788     0.765        33\n",
            "          38      0.760     0.950     0.844        20\n",
            "          39      0.594     0.704     0.644        27\n",
            "          40      0.304     0.304     0.304        23\n",
            "          41      0.929     0.839     0.881        31\n",
            "          42      0.667     0.333     0.444        30\n",
            "          43      0.842     0.800     0.821        20\n",
            "          44      0.562     0.300     0.391        30\n",
            "          45      0.857     0.923     0.889        26\n",
            "          46      1.000     0.964     0.982        28\n",
            "          47      0.920     0.767     0.836        30\n",
            "          48      0.773     0.944     0.850        18\n",
            "          49      0.808     0.778     0.792        27\n",
            "          50      0.619     0.722     0.667        18\n",
            "          51      0.538     0.609     0.571        23\n",
            "          52      0.480     0.480     0.480        25\n",
            "          53      0.373     0.655     0.475        29\n",
            "          54      0.667     0.562     0.610        32\n",
            "          55      0.333     0.444     0.381        18\n",
            "          56      0.731     0.826     0.776        23\n",
            "          57      0.639     0.885     0.742        26\n",
            "          58      0.700     0.840     0.764        25\n",
            "          59      0.947     0.947     0.947        19\n",
            "          60      0.571     0.333     0.421        24\n",
            "          61      0.500     0.478     0.489        23\n",
            "          62      0.923     1.000     0.960        24\n",
            "          63      0.875     0.560     0.683        25\n",
            "          64      0.833     0.652     0.732        23\n",
            "          65      0.806     0.962     0.877        26\n",
            "          66      0.625     0.652     0.638        23\n",
            "          67      0.619     0.897     0.732        29\n",
            "          68      0.750     0.500     0.600        18\n",
            "          69      0.542     0.371     0.441        35\n",
            "          70      0.500     0.692     0.581        26\n",
            "          71      0.867     0.722     0.788        18\n",
            "          72      0.857     0.643     0.735        28\n",
            "          73      0.526     0.435     0.476        23\n",
            "          74      0.815     0.846     0.830        26\n",
            "          75      0.842     0.667     0.744        24\n",
            "          76      0.622     0.742     0.676        31\n",
            "          77      0.762     0.762     0.762        21\n",
            "          78      0.593     0.571     0.582        28\n",
            "          79      0.885     0.719     0.793        32\n",
            "          80      0.857     0.947     0.900        19\n",
            "          81      0.850     0.773     0.810        22\n",
            "          82      0.680     0.739     0.708        23\n",
            "          83      0.958     0.920     0.939        25\n",
            "          84      0.704     0.826     0.760        23\n",
            "          85      0.800     0.833     0.816        24\n",
            "          86      0.585     0.857     0.696        28\n",
            "          87      0.571     0.522     0.545        23\n",
            "          88      0.852     0.920     0.885        25\n",
            "          89      0.565     0.722     0.634        18\n",
            "          90      0.553     1.000     0.712        26\n",
            "          91      0.789     0.682     0.732        22\n",
            "          92      0.944     1.000     0.971        17\n",
            "          93      1.000     1.000     1.000        29\n",
            "          94      0.750     0.960     0.842        25\n",
            "          95      0.862     0.926     0.893        27\n",
            "          96      0.543     0.826     0.655        23\n",
            "          97      0.833     0.577     0.682        26\n",
            "          98      0.630     0.680     0.654        25\n",
            "          99      0.810     0.739     0.773        23\n",
            "\n",
            "    accuracy                          0.694      2500\n",
            "   macro avg      0.703     0.695     0.688      2500\n",
            "weighted avg      0.704     0.694     0.688      2500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "result_report= classification_report(true_labels_str, predicted_labels_str, digits=3)\n",
        "print(result_report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4969aebe44e847919be543832d4ad315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_096919bad2124706945f1eb3a23a8b59",
              "IPY_MODEL_d02f5380531842eeb32aed7d19619191",
              "IPY_MODEL_fb8a825f08274622b27630b692f3e391"
            ],
            "layout": "IPY_MODEL_8516e09454ef469f936a3de29852313e"
          }
        },
        "096919bad2124706945f1eb3a23a8b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d39b5f4647a4f8997c18ce131f7b19d",
            "placeholder": "​",
            "style": "IPY_MODEL_394fb61cf02245d0a8898373130d5575",
            "value": "Downloading: 100%"
          }
        },
        "d02f5380531842eeb32aed7d19619191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887702e5c2524db6959786970b4db6ea",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1d1e23e16c34f6eb9f11e6e43e451ac",
            "value": 898823
          }
        },
        "fb8a825f08274622b27630b692f3e391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a71e293a98041dbbe48cc9211c384e6",
            "placeholder": "​",
            "style": "IPY_MODEL_939f42a01d87450b8a6606b6bd6d0a29",
            "value": " 899k/899k [00:00&lt;00:00, 2.80MB/s]"
          }
        },
        "8516e09454ef469f936a3de29852313e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d39b5f4647a4f8997c18ce131f7b19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394fb61cf02245d0a8898373130d5575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "887702e5c2524db6959786970b4db6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d1e23e16c34f6eb9f11e6e43e451ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a71e293a98041dbbe48cc9211c384e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939f42a01d87450b8a6606b6bd6d0a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b269a0ac4f104b6bbb13edd4d2242a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3759e05c95404a369b48d21d5a5dfa4d",
              "IPY_MODEL_ee20ccb6f89b4092bc876e15f57f0134",
              "IPY_MODEL_2f50cbc5bdce46768e769fc567a7adfb"
            ],
            "layout": "IPY_MODEL_9350128afb80485f89c45251b62978eb"
          }
        },
        "3759e05c95404a369b48d21d5a5dfa4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b578ce557b24078a8a58c2d0ddaa43c",
            "placeholder": "​",
            "style": "IPY_MODEL_95667facad5d4d578966b09089bec9f6",
            "value": "Downloading: 100%"
          }
        },
        "ee20ccb6f89b4092bc876e15f57f0134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7566f8c846f541aeb90d8a197a99fcd6",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_548392dcb5de4348bf5fcd889734480d",
            "value": 456318
          }
        },
        "2f50cbc5bdce46768e769fc567a7adfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee8cc6a0eb5a4a20a92e85b60ebf8940",
            "placeholder": "​",
            "style": "IPY_MODEL_431b6aa8f4934bbaa23022861738e3cb",
            "value": " 456k/456k [00:00&lt;00:00, 822kB/s]"
          }
        },
        "9350128afb80485f89c45251b62978eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b578ce557b24078a8a58c2d0ddaa43c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95667facad5d4d578966b09089bec9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7566f8c846f541aeb90d8a197a99fcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "548392dcb5de4348bf5fcd889734480d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee8cc6a0eb5a4a20a92e85b60ebf8940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "431b6aa8f4934bbaa23022861738e3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42060f409d164d0895154ef2b36f53bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58d7d23aa7d448439fb7987e6fa610f5",
              "IPY_MODEL_1f9df558724d40ecadb90492ea1a09ab",
              "IPY_MODEL_d290ec0df19742a7b1511e1012db246c"
            ],
            "layout": "IPY_MODEL_ea274e5a6f514a7ea46a71717f7dcfd7"
          }
        },
        "58d7d23aa7d448439fb7987e6fa610f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_732877d12ef04c07b4a5669cf5b0ed31",
            "placeholder": "​",
            "style": "IPY_MODEL_25d9455f2b94428096903b1da3705b6c",
            "value": "Downloading: 100%"
          }
        },
        "1f9df558724d40ecadb90492ea1a09ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_085372f681694020b12c55ea1017525f",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bc5702cf040423594d2448de52c1793",
            "value": 481
          }
        },
        "d290ec0df19742a7b1511e1012db246c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eca7c4c21954e78a031c8d884146b30",
            "placeholder": "​",
            "style": "IPY_MODEL_6fdf6475ef484b1189635dd9aedb4a54",
            "value": " 481/481 [00:00&lt;00:00, 13.0kB/s]"
          }
        },
        "ea274e5a6f514a7ea46a71717f7dcfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732877d12ef04c07b4a5669cf5b0ed31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d9455f2b94428096903b1da3705b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "085372f681694020b12c55ea1017525f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc5702cf040423594d2448de52c1793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2eca7c4c21954e78a031c8d884146b30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fdf6475ef484b1189635dd9aedb4a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35e67e07531499e91e85d539d486ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2921150ad6e34551a4e5fa7f9064caeb",
              "IPY_MODEL_9e76fea7a493463ab4abffbe3f0a25af",
              "IPY_MODEL_684bbe50d5ce45699694360c0f77c162"
            ],
            "layout": "IPY_MODEL_844591b0d8864f9081536ec03bd0e550"
          }
        },
        "2921150ad6e34551a4e5fa7f9064caeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53964fbcf0164b4e915265e644534461",
            "placeholder": "​",
            "style": "IPY_MODEL_8fd81671d387412b99f3a0eac97b7384",
            "value": "Downloading: 100%"
          }
        },
        "9e76fea7a493463ab4abffbe3f0a25af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7507a99cc17746f2a2d332e67a9fa25d",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2c77ff7c76f4b628a08d0e57913889d",
            "value": 501200538
          }
        },
        "684bbe50d5ce45699694360c0f77c162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15daeab1079b425790b9d09d304c010c",
            "placeholder": "​",
            "style": "IPY_MODEL_a67ca474f75c41d0a65f8fbf83591c20",
            "value": " 501M/501M [00:09&lt;00:00, 56.3MB/s]"
          }
        },
        "844591b0d8864f9081536ec03bd0e550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53964fbcf0164b4e915265e644534461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd81671d387412b99f3a0eac97b7384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7507a99cc17746f2a2d332e67a9fa25d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c77ff7c76f4b628a08d0e57913889d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15daeab1079b425790b9d09d304c010c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67ca474f75c41d0a65f8fbf83591c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
